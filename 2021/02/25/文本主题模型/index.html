<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考刘建平的博客。 在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是">
<meta property="og:type" content="article">
<meta property="og:title" content="文本主题模型">
<meta property="og:url" content="http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="CAKGOD">
<meta property="og:description" content="本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考刘建平的博客。 在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170505121313664-221059394.png">
<meta property="article:published_time" content="2021-02-25T08:59:00.000Z">
<meta property="article:modified_time" content="2021-02-25T10:04:37.120Z">
<meta property="article:author" content="CAKGOD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>文本主题模型</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="CAKGOD" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" href="/2021/02/25/word2vec/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&text=文本主题模型"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&is_video=false&description=文本主题模型"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=文本主题模型&body=Check out this article: http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&name=文本主题模型&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&t=文本主题模型"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95%EF%BC%88LSI%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">潜在语义索引（LSI）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">非负矩阵分解（NMF）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-number">3.</span> <span class="toc-text">LDA</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        文本主题模型
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">CAKGOD</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-02-25T08:59:00.000Z" itemprop="datePublished">2021-02-25</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/">刘建平</a>的博客。</p>
<p>在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。</p>
<p>在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。</p>
<p>聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。</p>
<p>那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。</p>
<p>当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。</p>
<h2 id="潜在语义索引（LSI）"><a href="#潜在语义索引（LSI）" class="headerlink" title="潜在语义索引（LSI）"></a>潜在语义索引（LSI）</h2><p>潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic  Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。</p>
<p>这里我们简要回顾下SVD：对于一个$m×n$的矩阵$A$，可以分解为下面三个矩阵：</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}</script><p>有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$应第j个词和第m个词义的相关度。$Σ_{lm}$对应第l个主题和第m个词义的相关度。</p>
<p>也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$Σ_{lm}$对应第l个词义和第m个主题的相关度。</p>
<p>这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。</p>
<p>下面介绍一个简单的例子，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png" alt="img"></p>
<p>这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png" alt="img"></p>
<p>从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。</p>
<p>在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ：</p>
<script type="math/tex; mode=display">
sim(d1,d2) = \frac{(-0.4945)*(-0.6458) + (0.6492)*(-0.7194)}{\sqrt{(-0.4945)^2+0.6492^2}\sqrt{(-0.6458)^2+(-0.7194)^2}}</script><p>对LSI模型进行总结，LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。</p>
<p>1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。</p>
<p>2） 主题值的选取对结果的影响非常大，很难选择合适的k值。</p>
<p>3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。</p>
<p>对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。</p>
<p>回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。</p>
<h2 id="非负矩阵分解（NMF）"><a href="#非负矩阵分解（NMF）" class="headerlink" title="非负矩阵分解（NMF）"></a>非负矩阵分解（NMF）</h2><p>前面讲到了LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？</p>
<p>非负矩阵分解(non-negative matrix factorization，以下简称NMF)是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识别，语音识别等，这里我们会主要关注于它在文本主题模型里的运用。</p>
<p>回顾奇异值分解，它会将一个矩阵分解为三个矩阵：</p>
<script type="math/tex; mode=display">
A = U\Sigma V^T</script><p>如果降维到$k$维，则表达式为：</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>但是NMF虽然也是矩阵分解，它却使用了不同的思路，它的目标是期望将矩阵分解为两个矩阵:</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx  W_{m \times k}H_{k \times n}</script><p>下面讲解NMF的优化思路，NMF期望找到这样的两个矩阵$W,H$，使$WH$的矩阵乘积得到的矩阵对应的每个位置的值和原矩阵$A$对应位置的值相比误差尽可能的小。用数学的语言表示就是：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}\sum\limits_{i,j}(A_{ij}-(WH)_{ij})^2</script><p>如果完全用矩阵表示，则为：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}||A-WH||_{Fro}^2</script><p>其中，$||∗||_{Fro}$为Frobenius范数。</p>
<p>当然对于这个式子，我们也可以加上L1和L2的正则化项如下：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}||A-WH||_{Fro}^2 +\alpha\rho|| W||_1+\alpha\rho|| H||_1+\frac{\alpha(1-\rho)}{2}|| W||_{Fro}^2 + \frac{\alpha(1-\rho)}{2}|| H||_{Fro}^2</script><p>其中，$α$为L1&amp;L2正则化参数，而$ρ$为L1正则化占总正则化项的比例。$||∗||_1$为L1范数。</p>
<p>我们要求解的有$m∗k+k∗n$个参数。参数不少，常用的迭代方法有梯度下降法和拟牛顿法。不过如果我们决定加上了L1正则化的话就不能用梯度下降和拟牛顿法了。此时可以用坐标轴下降法或者最小角回归法来求解。scikit-learn中NMF的库目前是使用坐标轴下降法来求解的，即在迭代时，一次固定$m∗k+k∗n−1$个参数，仅仅最优化一个参数。</p>
<p>NMF矩阵分解如何运用到我们的主题模型呢？</p>
<p>此时NMF可以这样解释：我们输入的有m个文本，n个词，而$A_{ij}$对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个文本的和第k个主题的概率相关度，而$H_{kj}$对应第j个词和第k个主题的概率相关度。</p>
<p>当然也可以反过来去解释：我们输入的有m个词，n个文本，而$A_{ij}$对应第i个词的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个词的和第k个主题的概率相关度，而$H_{kj}$对应第j个文本和第k个主题的概率相关度。</p>
<p>注意到这里我们使用的是”概率相关度”，这是因为我们使用的是”非负”的矩阵分解，这样我们的$W,H$矩阵值的大小可以用概率值的角度去看。从而可以得到文本和主题的概率分布关系。第二种解释用一个图来表示如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170505121313664-221059394.png" alt="img"></p>
<p>和LSI相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。当然NMF由于是两个矩阵，相比LSI的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价。</p>
<p>在 scikit-learn中，NMF在sklearn.decomposition.NMF包中，它支持L1和L2的正则化，而$W,H$的求解使用坐标轴下降法来实现。示例代码如下：</p>
<pre><code class="lang-python">import numpy as np
X = np.array([[1,1,5,2,3], [0,6,2,1,1], [3, 4,0,3,1], [4, 1,5,6,3]])
from sklearn.decomposition import NMF
model = NMF(n_components=2, alpha=0.01)
W = model.fit_transform(X)
H = model.components_
</code></pre>
<p>对NMF模型进行总结，NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本识别不一定很准确。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95%EF%BC%88LSI%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">潜在语义索引（LSI）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">非负矩阵分解（NMF）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-number">3.</span> <span class="toc-text">LDA</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&text=文本主题模型"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&is_video=false&description=文本主题模型"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=文本主题模型&body=Check out this article: http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&name=文本主题模型&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&t=文本主题模型"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2021-2025
    CAKGOD
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
