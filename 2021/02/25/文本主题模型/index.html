<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考刘建平的博客。 在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是">
<meta property="og:type" content="article">
<meta property="og:title" content="文本主题模型">
<meta property="og:url" content="http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="CAKGOD">
<meta property="og:description" content="本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考刘建平的博客。 在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170505121313664-221059394.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170517135228963-491669544.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170517134339588-825441177.png">
<meta property="article:published_time" content="2021-02-25T08:59:00.000Z">
<meta property="article:modified_time" content="2021-02-26T02:32:37.791Z">
<meta property="article:author" content="CAKGOD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>文本主题模型</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="CAKGOD" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2021/02/26/%E5%85%B8%E5%9E%8B%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%EF%BC%88CCA%EF%BC%89/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2021/02/25/word2vec/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&text=文本主题模型"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&is_video=false&description=文本主题模型"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=文本主题模型&body=Check out this article: http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&name=文本主题模型&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&t=文本主题模型"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95%EF%BC%88LSI%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">潜在语义索引（LSI）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">非负矩阵分解（NMF）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-number">3.</span> <span class="toc-text">LDA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">LDA贝叶斯模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E4%B8%8EBeta%E5%88%86%E5%B8%83"><span class="toc-number">3.2.</span> <span class="toc-text">二项分布与Beta分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83%E4%B8%8EDirichlet-%E5%88%86%E5%B8%83"><span class="toc-number">3.3.</span> <span class="toc-text">多项分布与Dirichlet 分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.4.</span> <span class="toc-text">LDA主题模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA%E6%B1%82%E8%A7%A3%E4%B9%8BGibbs%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">LDA求解之Gibbs采样算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA%E6%B1%82%E8%A7%A3%E4%B9%8B%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%ADEM%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">LDA求解之变分推断EM算法</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        文本主题模型
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">CAKGOD</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-02-25T08:59:00.000Z" itemprop="datePublished">2021-02-25</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/">刘建平</a>的博客。</p>
<p>在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。</p>
<p>在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。</p>
<p>聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。</p>
<p>那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。</p>
<p>当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。</p>
<h2 id="潜在语义索引（LSI）"><a href="#潜在语义索引（LSI）" class="headerlink" title="潜在语义索引（LSI）"></a>潜在语义索引（LSI）</h2><p>潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic  Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。</p>
<p>这里我们简要回顾下SVD：对于一个$m×n$的矩阵$A$，可以分解为下面三个矩阵：</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}</script><p>有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$应第j个词和第m个词义的相关度。$Σ_{lm}$对应第l个主题和第m个词义的相关度。</p>
<p>也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$Σ_{lm}$对应第l个词义和第m个主题的相关度。</p>
<p>这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。</p>
<p>下面介绍一个简单的例子，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png" alt="img"></p>
<p>这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png" alt="img"></p>
<p>从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。</p>
<p>在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ：</p>
<script type="math/tex; mode=display">
sim(d1,d2) = \frac{(-0.4945)*(-0.6458) + (0.6492)*(-0.7194)}{\sqrt{(-0.4945)^2+0.6492^2}\sqrt{(-0.6458)^2+(-0.7194)^2}}</script><p>对LSI模型进行总结，LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。</p>
<p>1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。</p>
<p>2） 主题值的选取对结果的影响非常大，很难选择合适的k值。</p>
<p>3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。</p>
<p>对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。</p>
<p>回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。</p>
<h2 id="非负矩阵分解（NMF）"><a href="#非负矩阵分解（NMF）" class="headerlink" title="非负矩阵分解（NMF）"></a>非负矩阵分解（NMF）</h2><p>前面讲到了LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？</p>
<p>非负矩阵分解(non-negative matrix factorization，以下简称NMF)是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识别，语音识别等，这里我们会主要关注于它在文本主题模型里的运用。</p>
<p>回顾奇异值分解，它会将一个矩阵分解为三个矩阵：</p>
<script type="math/tex; mode=display">
A = U\Sigma V^T</script><p>如果降维到$k$维，则表达式为：</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>但是NMF虽然也是矩阵分解，它却使用了不同的思路，它的目标是期望将矩阵分解为两个矩阵:</p>
<script type="math/tex; mode=display">
A_{m \times n} \approx  W_{m \times k}H_{k \times n}</script><p>下面讲解NMF的优化思路，NMF期望找到这样的两个矩阵$W,H$，使$WH$的矩阵乘积得到的矩阵对应的每个位置的值和原矩阵$A$对应位置的值相比误差尽可能的小。用数学的语言表示就是：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}\sum\limits_{i,j}(A_{ij}-(WH)_{ij})^2</script><p>如果完全用矩阵表示，则为：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}||A-WH||_{Fro}^2</script><p>其中，$||∗||_{Fro}$为Frobenius范数。</p>
<p>当然对于这个式子，我们也可以加上L1和L2的正则化项如下：</p>
<script type="math/tex; mode=display">
\underbrace{arg\;min}_{W,H}\frac{1}{2}||A-WH||_{Fro}^2 +\alpha\rho|| W||_1+\alpha\rho|| H||_1+\frac{\alpha(1-\rho)}{2}|| W||_{Fro}^2 + \frac{\alpha(1-\rho)}{2}|| H||_{Fro}^2</script><p>其中，$α$为L1&amp;L2正则化参数，而$ρ$为L1正则化占总正则化项的比例。$||∗||_1$为L1范数。</p>
<p>我们要求解的有$m∗k+k∗n$个参数。参数不少，常用的迭代方法有梯度下降法和拟牛顿法。不过如果我们决定加上了L1正则化的话就不能用梯度下降和拟牛顿法了。此时可以用坐标轴下降法或者最小角回归法来求解。scikit-learn中NMF的库目前是使用坐标轴下降法来求解的，即在迭代时，一次固定$m∗k+k∗n−1$个参数，仅仅最优化一个参数。</p>
<p>NMF矩阵分解如何运用到我们的主题模型呢？</p>
<p>此时NMF可以这样解释：我们输入的有m个文本，n个词，而$A_{ij}$对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个文本的和第k个主题的概率相关度，而$H_{kj}$对应第j个词和第k个主题的概率相关度。</p>
<p>当然也可以反过来去解释：我们输入的有m个词，n个文本，而$A_{ij}$对应第i个词的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个词的和第k个主题的概率相关度，而$H_{kj}$对应第j个文本和第k个主题的概率相关度。</p>
<p>注意到这里我们使用的是”概率相关度”，这是因为我们使用的是”非负”的矩阵分解，这样我们的$W,H$矩阵值的大小可以用概率值的角度去看。从而可以得到文本和主题的概率分布关系。第二种解释用一个图来表示如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170505121313664-221059394.png" alt="img"></p>
<p>和LSI相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。当然NMF由于是两个矩阵，相比LSI的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价。</p>
<p>在 scikit-learn中，NMF在sklearn.decomposition.NMF包中，它支持L1和L2的正则化，而$W,H$的求解使用坐标轴下降法来实现。示例代码如下：</p>
<pre><code class="lang-python">import numpy as np
X = np.array([[1,1,5,2,3], [0,6,2,1,1], [3, 4,0,3,1], [4, 1,5,6,3]])
from sklearn.decomposition import NMF
model = NMF(n_components=2, alpha=0.01)
W = model.fit_transform(X)
H = model.components_
</code></pre>
<p>对NMF模型进行总结，NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本识别不一定很准确。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet Allocation，以下简称LDA)。注意机器学习还有一个LDA，即线性判别分析，主要是用于降维和分类的，如果大家需要了解这个LDA的信息，参看之前写的<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6244265.html">线性判别分析LDA原理总结</a>。文本关注于隐含狄利克雷分布对应的LDA。</p>
<h3 id="LDA贝叶斯模型"><a href="#LDA贝叶斯模型" class="headerlink" title="LDA贝叶斯模型"></a>LDA贝叶斯模型</h3><p>LDA是基于贝叶斯模型的，涉及到贝叶斯模型离不开“先验分布”，“数据（似然）”和”后验分布”三块。在<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6069267.html">朴素贝叶斯算法原理小结</a>中我们也已经讲到了这套贝叶斯理论。在贝叶斯学派这里：</p>
<script type="math/tex; mode=display">
先验分布 + 数据（似然）= 后验分布</script><p>这点其实很好理解，因为这符合我们人的思维方式，比如你对好人和坏人的认知，先验分布为：100个好人和100个的坏人，即你认为好人坏人各占一半，现在你被2个好人（数据）帮助了和1个坏人骗了，于是你得到了新的后验分布为：102个好人和101个的坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成你的新的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布为：103个好人和104个的坏人。依次继续更新下去。</p>
<h3 id="二项分布与Beta分布"><a href="#二项分布与Beta分布" class="headerlink" title="二项分布与Beta分布"></a>二项分布与Beta分布</h3><p>贝叶斯模型和认知过程，假如用数学和概率的方式该如何表达呢？对于我们的数据（似然），这个好办，用一个二项分布就可以搞定，即对于二项分布：</p>
<script type="math/tex; mode=display">
Binom(k|n,p) = {n \choose k}p^k(1-p)^{n-k}</script><p>其中p我们可以理解为好人的概率，k为好人的个数，n为好人坏人的总数。虽然数据(似然)很好理解，但是对于先验分布，我们就要费一番脑筋了，为什么呢？因为我们希望这个先验分布和数据（似然）对应的二项分布集合后，得到的后验分布在后面还可以作为先验分布！就像上面例子里的“102个好人和101个的坏人”，它是前面一次贝叶斯推荐的后验分布，又是后一次贝叶斯推荐的先验分布。也即是说，我们希望先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫<strong>共轭分布</strong>。在我们的例子里，我们希望找到和二项分布共轭的分布。和二项分布共轭的分布其实就是Beta分布。Beta分布的表达式为：</p>
<script type="math/tex; mode=display">
Beta(p|\alpha,\beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^NaN</script><p>其中$Γ$是Gamma函数，满足$Γ(x)=(x−1)!$</p>
<p>仔细观察Beta分布和二项分布，可以发现两者的密度函数很相似，区别仅仅在前面的归一化的阶乘项。那么它如何做到先验分布和后验分布的形式一样呢？后验分布$P(p|n,k,α,β)$推导如下：</p>
<script type="math/tex; mode=display">
\begin{align} P(p|n,k,\alpha,\beta) & \propto P(k|n,p)P(p|\alpha,\beta) \\ & = P(k|n,p)P(p|\alpha,\beta) \\& = Binom(k|n,p) Beta(p|\alpha,\beta) \\ &= {n \choose k}p^k(1-p)^{n-k} \times  \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^NaN \\& \propto p^{k+\alpha-1}(1-p)^{n-k + \beta -1}　 \end{align}</script><p>将上面最后的式子归一化以后，得到我们的后验概率为：</p>
<script type="math/tex; mode=display">
P(p|n,k,\alpha,\beta) = \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha + k)\Gamma(\beta + n - k)}p^{k+\alpha-1}(1-p)^{n-k + \beta -1}</script><p>可见我们的后验分布的确是Beta分布，而且我们发现：</p>
<script type="math/tex; mode=display">
Beta(p|\alpha,\beta) + BinomCount(k,n-k) = Beta(p|\alpha + k,\beta +n-k)</script><p>这个式子完全符合我们在上一节好人坏人例子里的情况，我们的认知会把数据里的好人坏人数分别加到我们的先验分布上，得到后验分布。　</p>
<p>我们在来看看Beta分布$Beta(p|α,β)$的期望:</p>
<script type="math/tex; mode=display">
\begin{align} E(Beta(p|\alpha,\beta)) & = \int_{0}^{1}tBeta(p|\alpha,\beta)dt \\& =  \int_{0}^{1}t \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha-1}(1-t)^NaNdt \\& = \int_{0}^{1}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}t^{\alpha}(1-t)^NaNdt \end{align}</script><p>由于上式最右边的乘积对应Beta分布$Beta(p|α+1,β)$,因此有：</p>
<script type="math/tex; mode=display">
\int_{0}^{1}\frac{\Gamma(\alpha + \beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)}p^{\alpha}(1-p)^NaN dp=1</script><p>这样我们的期望可以表达为：</p>
<script type="math/tex; mode=display">
E(Beta(p|\alpha,\beta)) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha + \beta+1)} =  \frac{\alpha}{\alpha + \beta}</script><h3 id="多项分布与Dirichlet-分布"><a href="#多项分布与Dirichlet-分布" class="headerlink" title="多项分布与Dirichlet 分布"></a>多项分布与Dirichlet 分布</h3><p>现在我们回到上面好人坏人的问题，假如我们发现有第三类人，不好不坏的人，这时候我们如何用贝叶斯来表达这个模型分布呢？之前我们是二维分布，现在是三维分布。由于二维我们使用了Beta分布和二项分布来表达这个模型，则在三维时，以此类推，我们可以用三维的Beta分布来表达先验后验分布，三项的多项分布来表达数据（似然）。</p>
<p>三项的多项分布好表达，我们假设数据中的第一类有$m1$个好人，第二类有$m2$个坏人，第三类为$m3=n−m1−m2$个不好不坏的人,对应的概率分别为$p1,p2,p3=1−p1−p2$，则对应的多项分布为：</p>
<script type="math/tex; mode=display">
multi(m_1,m_2,m_3|n,p_1,p_2,p_3) = \frac{n!}{m_1! m_2!m_3!}p_1^{m_1}p_2^{m_2}p_3^{m_3}</script><p>那三维的Beta分布呢？超过二维的Beta分布我们一般称之为狄利克雷(以下称为Dirichlet )分布。也可以说Beta分布是Dirichlet 分布在二维时的特殊形式。从二维的Beta分布表达式，我们很容易写出三维的Dirichlet分布如下：</p>
<script type="math/tex; mode=display">
Dirichlet(p_1,p_2,p_3|\alpha_1,\alpha_2, \alpha_3) = \frac{\Gamma(\alpha_1+ \alpha_2 + \alpha_3)}{\Gamma(\alpha_1)\Gamma(\alpha_2)\Gamma(\alpha_3)}p_1^{\alpha_1-1}(p_2)^{\alpha_2-1}(p_3)^{\alpha_3-1}</script><p>同样的方法，我们可以写出4维，5维，。。。以及更高维的Dirichlet 分布的概率密度函数。为了简化表达式，我们用向量来表示概率和计数,这样多项分布可以表示为：$Dirichlet(p⃗ |α⃗ )$,而多项分布可以表示为：$multi(m⃗ |n,p⃗ )$。</p>
<p>一般意义上的K维Dirichlet 分布表达式为：</p>
<script type="math/tex; mode=display">
Dirichlet(\vec p| \vec \alpha) = \frac{\Gamma(\sum\limits_{k=1}^K\alpha_k)}{\prod_{k=1}^K\Gamma(\alpha_k)}\prod_{k=1}^Kp_k^{\alpha_k-1}</script><p>而多项分布和Dirichlet 分布也满足共轭关系，这样我们可以得到和上一节类似的结论：</p>
<script type="math/tex; mode=display">
Dirichlet(\vec p|\vec \alpha) + MultiCount(\vec m) = Dirichlet(\vec p|\vec \alpha + \vec m)</script><p>对于Dirichlet 分布的期望，也有和Beta分布类似的性质：</p>
<script type="math/tex; mode=display">
E(Dirichlet(\vec p|\vec \alpha)) = (\frac{\alpha_1}{\sum\limits_{k=1}^K\alpha_k}, \frac{\alpha_2}{\sum\limits_{k=1}^K\alpha_k},...,\frac{\alpha_K}{\sum\limits_{k=1}^K\alpha_k})</script><h3 id="LDA主题模型"><a href="#LDA主题模型" class="headerlink" title="LDA主题模型"></a>LDA主题模型</h3><p>我们的问题是这样的，我们有MM篇文档，对应第d个文档中有有NdNd个词。即输入为如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170517135228963-491669544.png" alt="img"></p>
<p>我们的目标是找到每一篇文档的主题分布和每一个主题中词的分布。在LDA模型中，我们需要先假定一个主题数目$K$，这样所有的分布就都基于$K$个主题展开。那么具体LDA模型是怎么样的呢？具体如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170517134339588-825441177.png" alt="img"></p>
<p>LDA假设文档主题的先验分布是Dirichlet分布，即对于任一文档$d$, 其主题分布$θd$为：</p>
<script type="math/tex; mode=display">
\theta_d = Dirichlet(\vec \alpha)</script><p>其中，$α$为分布的超参数，是一个$K$维向量。</p>
<p>LDA假设主题中词的先验分布是Dirichlet分布，即对于任一主题$k$, 其词分布$β_k$为：</p>
<script type="math/tex; mode=display">
\beta_k= Dirichlet(\vec \eta)</script><p>其中，$η$为分布的超参数，是一个$V$维向量。$V$代表词汇表里所有词的个数。</p>
<p>对于数据中任一一篇文档$d$中的第$n$个词，我们可以从主题分布$θ_d$中得到它的主题编号$z_{dn}$的分布为：</p>
<script type="math/tex; mode=display">
z_{dn} = multi(\theta_d)</script><p>而对于该主题编号，得到我们看到的词$w_{dn}$的概率分布为： </p>
<script type="math/tex; mode=display">
w_{dn} = multi(\beta_{z_{dn}})</script><p>理解LDA主题模型的主要任务就是理解上面的这个模型。这个模型里，我们有$M$个文档主题的Dirichlet分布，而对应的数据有$M$个主题编号的多项分布，这样$(α→θ_d→{z⃗}_d)$就组成了Dirichlet-multi共轭，可以使用前面提到的贝叶斯推断的方法得到基于Dirichlet分布的文档主题后验分布。</p>
<p>如果在第d个文档中，第k个主题的词的个数为：$n_d^{(k)}$, 则对应的多项分布的计数可以表示为:</p>
<script type="math/tex; mode=display">
\vec n_d = (n_d^{(1)}, n_d^{(2)},...n_d^{(K)})</script><p>利用Dirichlet-multi共轭，得到$θ_d$的后验分布为：</p>
<script type="math/tex; mode=display">
Dirichlet(\theta_d | \vec \alpha + \vec n_d)</script><p>同样的道理，对于主题与词的分布，我们有$K$个主题与词的Dirichlet分布，而对应的数据有$K$个主题编号的多项分布，这样$(η→β_k→w⃗_{(k)}$就组成了Dirichlet-multi共轭，可以使用前面提到的贝叶斯推断的方法得到基于Dirichlet分布的主题词的后验分布。</p>
<p>如果在第k个主题中，第v个词的个数为：$n^{(v)}_k$, 则对应的多项分布的计数可以表示为:</p>
<script type="math/tex; mode=display">
\vec n_k = (n_k^{(1)}, n_k^{(2)},...n_k^{(V)})</script><p>利用Dirichlet-multi共轭，得到$β_k$的后验分布为：</p>
<script type="math/tex; mode=display">
Dirichlet(\beta_k | \vec \eta+ \vec n_k)</script><p>由于主题产生词不依赖具体某一个文档，因此文档主题分布和主题词分布是独立的。理解了上面这$M+K$组Dirichlet-multi共轭，就理解了LDA的基本原理了。</p>
<p>现在的问题是，基于这个LDA模型如何求解我们想要的每一篇文档的主题分布和每一个主题中词的分布呢？一般有两种方法，第一种是基于Gibbs采样算法求解，第二种是基于变分推断EM算法求解。</p>
<h2 id="LDA求解之Gibbs采样算法"><a href="#LDA求解之Gibbs采样算法" class="headerlink" title="LDA求解之Gibbs采样算法"></a>LDA求解之Gibbs采样算法</h2><h2 id="LDA求解之变分推断EM算法"><a href="#LDA求解之变分推断EM算法" class="headerlink" title="LDA求解之变分推断EM算法"></a>LDA求解之变分推断EM算法</h2>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95%EF%BC%88LSI%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">潜在语义索引（LSI）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">非负矩阵分解（NMF）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA"><span class="toc-number">3.</span> <span class="toc-text">LDA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">LDA贝叶斯模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E4%B8%8EBeta%E5%88%86%E5%B8%83"><span class="toc-number">3.2.</span> <span class="toc-text">二项分布与Beta分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83%E4%B8%8EDirichlet-%E5%88%86%E5%B8%83"><span class="toc-number">3.3.</span> <span class="toc-text">多项分布与Dirichlet 分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.4.</span> <span class="toc-text">LDA主题模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA%E6%B1%82%E8%A7%A3%E4%B9%8BGibbs%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">LDA求解之Gibbs采样算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA%E6%B1%82%E8%A7%A3%E4%B9%8B%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%ADEM%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">LDA求解之变分推断EM算法</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&text=文本主题模型"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&is_video=false&description=文本主题模型"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=文本主题模型&body=Check out this article: http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&title=文本主题模型"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&name=文本主题模型&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/&t=文本主题模型"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2021-2025
    CAKGOD
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
