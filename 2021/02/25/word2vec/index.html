<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="转载自cnblogs.com&#x2F;pinard&#x2F;p&#x2F;7160330.html word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。 1">
<meta property="og:type" content="article">
<meta property="og:title" content="word2vec">
<meta property="og:url" content="http://example.com/2021/02/25/word2vec/index.html">
<meta property="og:site_name" content="CAKGOD">
<meta property="og:description" content="转载自cnblogs.com&#x2F;pinard&#x2F;p&#x2F;7160330.html word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。 1">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713145606275-2100371803.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713150625759-1047275185.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713151608181-1336632086.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713152436931-1817493891.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1042406/201812/1042406-20181205104643781-71258001.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105326843-18935623.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170728152731711-1136354166.png">
<meta property="article:published_time" content="2021-02-25T02:50:22.000Z">
<meta property="article:modified_time" content="2021-02-25T07:42:14.797Z">
<meta property="article:author" content="CAKGOD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713145606275-2100371803.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>word2vec</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="CAKGOD" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2021/02/25/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2021/02/25/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E6%A0%91/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/word2vec/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/word2vec/&text=word2vec"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/word2vec/&is_video=false&description=word2vec"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=word2vec&body=Check out this article: http://example.com/2021/02/25/word2vec/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/word2vec/&name=word2vec&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/word2vec/&t=word2vec"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">1、词向量基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81CBOW%E4%B8%8ESkip-Gram%E7%94%A8%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">2、CBOW与Skip-Gram用于神经网络语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81word2vec%E5%9F%BA%E7%A1%80%E4%B9%8B%E9%9C%8D%E5%A4%AB%E6%9B%BC%E6%A0%91"><span class="toc-number">3.</span> <span class="toc-text">3、word2vec基础之霍夫曼树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">4.</span> <span class="toc-text">3、基于Hierarchical Softmax的模型概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">5.</span> <span class="toc-text">4、基于Hierarchical Softmax的模型梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84CBOW%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.</span> <span class="toc-text">5、基于Hierarchical Softmax的CBOW模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84Skip-Gram%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.</span> <span class="toc-text">6、基于Hierarchical Softmax的Skip-Gram模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81Hierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">8.</span> <span class="toc-text">7、Hierarchical Softmax的模型源码和算法的对应</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81Hierarchical-Softmax%E7%9A%84%E7%BC%BA%E7%82%B9%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">9.</span> <span class="toc-text">8、Hierarchical Softmax的缺点与改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">10.</span> <span class="toc-text">9、基于Negative Sampling的模型概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">11.</span> <span class="toc-text">10、基于Negative Sampling的模型梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11%E3%80%81Negative-Sampling%E8%B4%9F%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">12.</span> <span class="toc-text">11、Negative Sampling负采样方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84CBOW%E6%A8%A1%E5%9E%8B"><span class="toc-number">13.</span> <span class="toc-text">12、基于Negative Sampling的CBOW模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84Skip-Gram%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">13、基于Negative Sampling的Skip-Gram模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14%E3%80%81Negative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">15.</span> <span class="toc-text">14、Negative Sampling的模型源码和算法的对应</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        word2vec
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">CAKGOD</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-02-25T02:50:22.000Z" itemprop="datePublished">2021-02-25</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>转载自cnblogs.com/pinard/p/7160330.html</p>
<p>word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的<a target="_blank" rel="noopener" href="https://github.com/tmikolov/word2vec">word2vec</a>代码为准。本文关注于word2vec的基础知识。</p>
<h2 id="1、词向量基础"><a href="#1、词向量基础" class="headerlink" title="1、词向量基础"></a>1、词向量基础</h2><p>用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713145606275-2100371803.png" alt="img"></p>
<p>One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>Distributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713150625759-1047275185.png" alt="img"></p>
<p>有了用Distributed Representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：</p>
<script type="math/tex; mode=display">
\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}</script><p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713151608181-1336632086.png" alt="img"></p>
<p>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<h2 id="2、CBOW与Skip-Gram用于神经网络语言模型"><a href="#2、CBOW与Skip-Gram用于神经网络语言模型" class="headerlink" title="2、CBOW与Skip-Gram用于神经网络语言模型"></a>2、CBOW与Skip-Gram用于神经网络语言模型</h2><p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。</p>
<p>CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713152436931-1817493891.png" alt="img"></p>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。</p>
<p>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词”Learning”是我们的输入，而这8个上下文词是我们的输出。</p>
<p>这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。</p>
<p>以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。</p>
<p>word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？</p>
<h2 id="3、word2vec基础之霍夫曼树"><a href="#3、word2vec基础之霍夫曼树" class="headerlink" title="3、word2vec基础之霍夫曼树"></a>3、word2vec基础之霍夫曼树</h2><p>word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</p>
<p>具体如何用霍夫曼树来进行CBOW和Skip-Gram的训练我们在下一节讲，这里我们先复习下霍夫曼树。</p>
<p>霍夫曼树的建立其实并不难，过程如下：</p>
<p>输入：权值为(w1,w2,…wn)(w1,w2,…wn)的nn个节点</p>
<p>输出：对应的霍夫曼树</p>
<p>1）将(w1,w2,…wn)(w1,w2,…wn)看做是有nn棵树的森林，每个树仅有一个节点。</p>
<p>2）在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。</p>
<p>3） 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。</p>
<p>4）重复步骤2）和3）直到森林里只有一棵树为止。</p>
<p>下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a,b,c,d,e,f)共6个节点，节点的权值分布是(20,4,8,6,16,3)。</p>
<p>首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是20,8,6,16,7。此时根节点权重最小的6,7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。</p>
<p><img src="https://img2018.cnblogs.com/blog/1042406/201812/1042406-20181205104643781-71258001.png" alt="img"></p>
<p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。</p>
<p>在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。</p>
<p>现在我们开始关注word2vec的语言模型如何改进传统的神经网络的方法。由于word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。</p>
<h2 id="3、基于Hierarchical-Softmax的模型概述"><a href="#3、基于Hierarchical-Softmax的模型概述" class="headerlink" title="3、基于Hierarchical Softmax的模型概述"></a>3、基于Hierarchical Softmax的模型概述</h2><p>我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中VV是词汇表的大小，</p>
<p><img src="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105326843-18935623.png" alt="img"></p>
<p>word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：$(1,2,3,4),(9,6,11,8),(5,10,7,12)$,那么我们word2vec映射后的词向量就是$(5,6,7,8)$。由于这里是从多个词向量变成了一个词向量。</p>
<p>第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。</p>
<p>由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词$w2$。</p>
<p><img src="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png" alt="img"></p>
<p>如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：</p>
<script type="math/tex; mode=display">
P(+) = \sigma(x_w^T\theta) = \frac{1}{1+e^{-x_w^T\theta}}</script><p>其中$x_w$是当前内部节点的词向量，而$θ$则是我们需要从训练样本求出的逻辑回归的模型参数。</p>
<p>使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为$V$现在变成了$log_2V$。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。</p>
<p>容易理解，被划分为左子树而成为负类的概率为$P(-) =  1-P(+)$。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看$P(−),P(+)$谁的概率值大。而控制$P(−),P(+)$谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$θ$。</p>
<p>对于上图中的$w_2$，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点$n(w_2,1)$的$p(-)$概率大，$n(w_2,2)$的$p(-)$概率大，$n(w_2,3)$的$p(+)$概率大。</p>
<p>回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$θ$, 使训练样本达到最大似然。那么如何达到最大似然呢？</p>
<h2 id="4、基于Hierarchical-Softmax的模型梯度计算"><a href="#4、基于Hierarchical-Softmax的模型梯度计算" class="headerlink" title="4、基于Hierarchical Softmax的模型梯度计算"></a>4、基于Hierarchical Softmax的模型梯度计算</h2><p>我们使用最大似然法来寻找所有节点的词向量和所有内部节点$θ$。先拿上面的$w2$例子来看，我们期望最大化下面的似然函数：</p>
<script type="math/tex; mode=display">
\prod_{i=1}^3P(n(w_i),i) = (1- \frac{1}{1+e^{-x_w^T\theta_1}})(1- \frac{1}{1+e^{-x_w^T\theta_2}})\frac{1}{1+e^{-x_w^T\theta_3}}</script><p>对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。</p>
<p>为了便于我们后面一般化的描述，我们定义输入的词为$w$,其从输入层词向量求和平均后的霍夫曼树根节点词向量为$x_w$, 从根节点到$w$所在的叶子节点，包含的节点总数为$l_w$, $w$在霍夫曼树中从根节点开始，经过的第$i$个节点表示为$p_i^w$,对应的霍夫曼编码为$d_i^w \in \{0,1\}$,其中$i=2,3,…l_w$。而该节点对应的模型参数表示为$\theta_i^w$, 其中$i=1,2,…l_w−1$，没有$i=l_w$是因为模型参数仅仅针对于霍夫曼树的内部节点。</p>
<p>定义$w$经过的霍夫曼树某一个节点$j$的逻辑回归概率为$P(d_j^w|x_w, \theta_{j-1}^w)$，其表达式为：</p>
<script type="math/tex; mode=display">
P(d_j^w|x_w, \theta_{j-1}^w)= \begin{cases}  \sigma(x_w^T\theta_{j-1}^w)& {d_j^w=0}\\ 1-  \sigma(x_w^T\theta_{j-1}^w) & {d_j^w = 1} \end{cases}</script><p>那么对于某一个目标输出词$w$,其最大似然为：</p>
<script type="math/tex; mode=display">
\prod_{j=2}^{l_w}P(d_j^w|x_w, \theta_{j-1}^w) = \prod_{j=2}^{l_w} [\sigma(x_w^T\theta_{j-1}^w)] ^{1-d_j^w}[1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w}</script><p>在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到$w$的对数似然函数$L$如下：</p>
<script type="math/tex; mode=display">
L= log \prod_{j=2}^{l_w}P(d_j^w|x_w, \theta_{j-1}^w) = \sum\limits_{j=2}^{l_w} ((1-d_j^w) log [\sigma(x_w^T\theta_{j-1}^w)]  + d_j^w log[1-\sigma(x_w^T\theta_{j-1}^w)])</script><p>要得到模型中$w$词向量和内部节点的模型参数$θ$, 我们使用梯度上升法即可。首先我们求模型参数$\theta_{j-1}^w$的梯度：</p>
<script type="math/tex; mode=display">
\begin{align} \frac{\partial L}{\partial \theta_{j-1}^w} & = (1-d_j^w)\frac{(\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w)}{\sigma(x_w^T\theta_{j-1}^w)}x_w - d_j^w \frac{(\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w)}{1- \sigma(x_w^T\theta_{j-1}^w)}x_w  \\ & =  (1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w -  d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w \\& = (1-d_j^w-\sigma(x_w^T\theta_{j-1}^w))x_w \end{align}</script><p>同样的方法，可以求出$x_w$的梯度表达式如下：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial x_w} = \sum\limits_{j=2}^{l_w}(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w))\theta_{j-1}^w</script><p>有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的$\theta_{j-1}^w$和$x_w$。</p>
<h2 id="5、基于Hierarchical-Softmax的CBOW模型"><a href="#5、基于Hierarchical-Softmax的CBOW模型" class="headerlink" title="5、基于Hierarchical Softmax的CBOW模型"></a>5、基于Hierarchical Softmax的CBOW模型</h2><p>由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。</p>
<p>首先我们要定义词向量的维度大小$M$，以及CBOW的上下文大小$2c$,这样我们对于训练样本中的每一个词，其前面的$c$个词和后面的$c$个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。</p>
<p>在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>对于从输入层到隐藏层（投影层），这一步比较简单，就是对$w$周围的$2c$个词向量求和取平均即可，即：</p>
<script type="math/tex; mode=display">
x_w = \frac{1}{2c}\sum\limits_{i=1}^{2c}x_i</script><p>第二步，通过梯度上升法来更新我们的$\theta_{j-1}^w$和$x_w$，注意这里的$x_w$是由$2c$个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个$x_i(i=1,2,,,,2c)$，即：</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w = \theta_{j-1}^w + \eta  (1-d_j^w-\sigma(x_w^T\theta_{j-1}^w))x_w</script><script type="math/tex; mode=display">
x_i= x_i +\eta  \sum\limits_{j=2}^{l_w}(1-d_j^w-\sigma(x_w^T\theta_{j-1}^w))\theta_{j-1}^w \;(i =1,2..,2c)</script><p>其中$η$为梯度上升法的步长。</p>
<p>这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>输入：基于CBOW的语料训练样本，词向量的维度大小$M$，CBOW的上下文大小$2c$,步长$η$</p>
<p>输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$w$</p>
<ol>
<li><p>基于语料训练样本建立霍夫曼树。</p>
</li>
<li><p>随机初始化所有的模型参数$θ$，所有的词向量$w$</p>
</li>
<li><p>进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：</p>
<p>3.1. e=0，计算$x_w= \frac{1}{2c}\sum\limits_{i=1}^{2c}x_i$</p>
<p>3.2. for j = 2 to $l_w$ 计算：</p>
<script type="math/tex; mode=display">
f = \sigma(x_w^T\theta_{j-1}^w)\\
g = (1-d_j^w-f)\eta\\
e = e + g\theta_{j-1}^w\\
\theta_{j-1}^w= \theta_{j-1}^w + gx_w</script><p>3.3. 对于$context(w)$中的每一个词向量$x_i$(共$2c$个)进行更新：</p>
<script type="math/tex; mode=display">
xi=xi+e</script><p>3.4. 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。</p>
</li>
</ol>
<h2 id="6、基于Hierarchical-Softmax的Skip-Gram模型"><a href="#6、基于Hierarchical-Softmax的Skip-Gram模型" class="headerlink" title="6、基于Hierarchical Softmax的Skip-Gram模型"></a>6、基于Hierarchical Softmax的Skip-Gram模型</h2><p>现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词$w$,输出的为$2c$c个词向量$context(w)$。</p>
<p>我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的$c$个词和后面的$c$个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。</p>
<p>Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。</p>
<p>在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即$x_w$就是词$w$对应的词向量。</p>
<p>第二步，通过梯度上升法来更新我们的$θ^w_{j−1}$和$x_w$，注意这里的$x_w$周围有$2c$个词向量，此时如果我们期望$P(x_i|x_w),i=1,2…2c$最大。此时我们注意到由于上下文是相互的，在期望$P(x_i|x_w),i=1,2…2c$最大化的同时，反过来我们也期望$P(x_w|x_i),i=1,2…2c$最大。那么是使用$P(x_i|x_w)$好还是$P(x_w|x_i)$好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新$x_w$一个词，而是$x_i,i=1,2…2c$共$2c$个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对$2c$个输出进行迭代更新。</p>
<p>这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法：</p>
<p>输入：基于Skip-Gram的语料训练样本，词向量的维度大小$M$，Skip-Gram的上下文大小$2c$,步长$η$</p>
<p>输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$w$</p>
<ol>
<li><p>基于语料训练样本建立霍夫曼树。</p>
</li>
<li><p>随机初始化所有的模型参数$θ$，所有的词向量$w$,</p>
</li>
<li><p>进行梯度上升迭代过程，对于训练集中的每一个样本$(w,context(w))$做如下处理：</p>
<p>3.1. for i =1 to $2c$:</p>
<p>　　　　i) e=0</p>
<p>​              ii) for j = 2 to $l_w$, 计算：</p>
<script type="math/tex; mode=display">
f = \sigma(x_i^T\theta_{j-1}^w)\\
g = (1-d_j^w-f)\eta\\
e = e + g\theta_{j-1}^w\\
\theta_{j-1}^w= \theta_{j-1}^w+ gx_i</script><p>​              iii) </p>
<script type="math/tex; mode=display">
x_i = x_i + e</script><p>3.2. 如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤3.1继续迭代。</p>
</li>
</ol>
<h2 id="7、Hierarchical-Softmax的模型源码和算法的对应"><a href="#7、Hierarchical-Softmax的模型源码和算法的对应" class="headerlink" title="7、Hierarchical Softmax的模型源码和算法的对应"></a>7、Hierarchical Softmax的模型源码和算法的对应</h2><p>这里给出上面算法和<a target="_blank" rel="noopener" href="https://github.com/tmikolov/word2vec/blob/master/word2vec.c">word2vec源码</a>中的变量对应关系。</p>
<p>在源代码中，基于Hierarchical Softmax的CBOW模型算法在435-463行，基于Hierarchical Softmax的Skip-Gram的模型算法在495-519行。大家可以对着源代码再深入研究下算法。</p>
<p>在源代码中，$neule$对应我们上面的$e$, $syn0$对应我们的$x_w$, $syn1$对应我们的$θ^i_{j−1}$, $layer1_size$对应词向量的维度，$window$对应我们的$c$。</p>
<p>另外，$vocab[word].code[d]$指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。</p>
<p>以上就是基于Hierarchical Softmax的word2vec模型，下一篇我们讨论基于Negative Sampling的word2vec模型。</p>
<h2 id="8、Hierarchical-Softmax的缺点与改进"><a href="#8、Hierarchical-Softmax的缺点与改进" class="headerlink" title="8、Hierarchical Softmax的缺点与改进"></a>8、Hierarchical Softmax的缺点与改进</h2><p>在讲基于Negative Sampling的word2vec模型前，我们先看看Hierarchical Softmax的的缺点。的确，使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？</p>
<p>Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解，下面我们就来看看Negative Sampling的求解思路.</p>
<h2 id="9、基于Negative-Sampling的模型概述"><a href="#9、基于Negative-Sampling的模型概述" class="headerlink" title="9、基于Negative Sampling的模型概述"></a>9、基于Negative Sampling的模型概述</h2><p>既然名字叫Negative Sampling（负采样），那么肯定使用了采样的方法。采样的方法有很多种，比如之前讲到的大名鼎鼎的MCMC。我们这里的Negative Sampling采样方法并没有MCMC那么复杂。</p>
<p>比如我们有一个训练样本，中心词是$w$,它周围上下文共有$2c$个词，记为$context(w)$。由于这个中心词$w$,的确和$context(w)$相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到$neg$个和$w$不同的中心词$w_i,i=1,2,..neg$，这样$context(w)$和$w_i$就组成了$neg$个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词$w_i$对应的模型参数$θ_i$，和每个词的词向量。</p>
<p>从上面的描述可以看出，Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。</p>
<p>不过有两个问题还需要弄明白：1）如果通过一个正例和neg个负例进行二元逻辑回归呢？ 2）如何进行负采样呢？</p>
<h2 id="10、基于Negative-Sampling的模型梯度计算"><a href="#10、基于Negative-Sampling的模型梯度计算" class="headerlink" title="10、基于Negative Sampling的模型梯度计算"></a>10、基于Negative Sampling的模型梯度计算</h2><p>Negative Sampling也是采用了二元逻辑回归来求解模型参数，通过负采样，我们得到了neg个负例$(context(w),w_i), i=1,2,..neg$。为了统一描述，我们将正例定义为$w_0$。</p>
<p>在逻辑回归中，我们的正例应该期望满足：</p>
<script type="math/tex; mode=display">
P(context(w_0), w_i) = \sigma(x_{w_0}^T\theta^{w_i}) ,y_i=1, i=0</script><p>我们的负例期望满足：</p>
<script type="math/tex; mode=display">
P(context(w_0), w_i) =1-  \sigma(x_{w_0}^T\theta^{w_i}), y_i = 0, i=1,2,..neg</script><p>我们期望可以最大化下式：</p>
<script type="math/tex; mode=display">
\prod_{i=0}^{neg}P(context(w_0), w_i) = \sigma(x_{w_0}^T\theta^{w_0})\prod_{i=1}^{neg}(1-  \sigma(x_{w_0}^T\theta^{w_i}))</script><p>利用<a target="_blank" rel="noopener" href="http://www.cnblogs.com/pinard/p/6029432.html">逻辑回归</a>和上一节的知识，我们容易写出此时模型的似然函数为：</p>
<script type="math/tex; mode=display">
\prod_{i=0}^{neg} \sigma(x_{w_0}^T\theta^{w_i})^{y_i}(1-  \sigma(x_{w_0}^T\theta^{w_i}))^{1-y_i}</script><p>和Hierarchical Softmax类似，我们采用随机梯度上升法，仅仅每次只用一个样本更新梯度，来进行迭代更新得到我们需要的$x_{wi},θ_{wi},i=0,1,..neg$, 这里我们需要求出$x_{w0},θ_{wi},i=0,1,..neg$的梯度。</p>
<p>首先我们计算$θ_{wi}$的梯度：</p>
<script type="math/tex; mode=display">
\begin{align} \frac{\partial L}{\partial \theta^{w_i} } &= y_i(1-  \sigma(x_{w_0}^T\theta^{w_i}))x_{w_0}-(1-y_i)\sigma(x_{w_0}^T\theta^{w_i})x_{w_0} \\ & = (y_i -\sigma(x_{w_0}^T\theta^{w_i})) x_{w_0} \end{align}</script><p>同样的方法，我们可以求出$x_{w0}$的梯度如下：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial x^{w_0} } = \sum\limits_{i=0}^{neg}(y_i -\sigma(x_{w_0}^T\theta^{w_i}))\theta^{w_i}</script><p>有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的$x_{w_0},θ_{w_i},i=0,1,..neg$。</p>
<h2 id="11、Negative-Sampling负采样方法"><a href="#11、Negative-Sampling负采样方法" class="headerlink" title="11、Negative Sampling负采样方法"></a>11、Negative Sampling负采样方法</h2><p>现在我们来看看如何进行负采样，得到neg个负例。word2vec采样的方法并不复杂，如果词汇表的大小为$V$,那么我们就将一段长度为1的线段分成$V$份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。每个词$w$的线段长度由下式决定：</p>
<script type="math/tex; mode=display">
len(w) = \frac{count(w)}{\sum\limits_{u \in vocab} count(u)}</script><p>在word2vec中，分子和分母都取了3/4次幂如下：</p>
<script type="math/tex; mode=display">
len(w) = \frac{count(w)^{3/4}}{\sum\limits_{u \in vocab} count(u)^{3/4}}</script><p>在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M&gt;&gt;V$，这样可以保证每个词对应的线段都会划分成对应的小块。而$M$份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从$M$个位置中采样出$neg$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。</p>
<p><img src="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170728152731711-1136354166.png" alt="img"></p>
<p>在word2vec中，MM取值默认为$10^8$。</p>
<h2 id="12、基于Negative-Sampling的CBOW模型"><a href="#12、基于Negative-Sampling的CBOW模型" class="headerlink" title="12、基于Negative Sampling的CBOW模型"></a>12、基于Negative Sampling的CBOW模型</h2><p>有了上面Negative Sampling负采样的方法和逻辑回归求解模型参数的方法，我们就可以总结出基于Negative Sampling的CBOW模型算法流程了。梯度迭代过程使用了随机梯度上升法：</p>
<p>输入：基于CBOW的语料训练样本，词向量的维度大小$Mcount$，CBOW的上下文大小$2c$,步长$η$, 负采样的个数neg</p>
<p>输出：词汇表每个词对应的模型参数$θ$，所有的词向量$x_w$</p>
<ol>
<li><p>随机初始化所有的模型参数$θ$，所有的词向量$w$</p>
</li>
<li><p>对于每个训练样本$(context(w_0),w_0)$,负采样出neg个负例中心词$w_i,i=1,2,…neg$</p>
</li>
<li><p>进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w_0),w_0,w_1,…w_neg)$做如下处理：</p>
<p>a). e=0， 计算$x_{w_0}= \frac{1}{2c}\sum\limits_{i=1}^{2c}x_i$</p>
<p>b). for i= 0 to neg, 计算：</p>
<script type="math/tex; mode=display">
f = \sigma(x_{w_0}^T\theta^{w_i})\\
g = (y_i-f)\eta\\
e = e + g\theta^{w_i}\\
\theta^{w_i}= \theta^{w_i} + gx_{w_0}</script><p>c). 对于$context(w)$中的每一个词向量$x_k$(共2c个)进行更新：</p>
<script type="math/tex; mode=display">
x_k = x_k + e</script><p>d). 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。</p>
</li>
<li></li>
</ol>
<h2 id="13、基于Negative-Sampling的Skip-Gram模型"><a href="#13、基于Negative-Sampling的Skip-Gram模型" class="headerlink" title="13、基于Negative Sampling的Skip-Gram模型"></a>13、基于Negative Sampling的Skip-Gram模型</h2><p>有了上一节CBOW的基础和上一篇基于Hierarchical Softmax的Skip-Gram模型基础，我们也可以总结出基于Negative Sampling的Skip-Gram模型算法流程了。梯度迭代过程使用了随机梯度上升法：</p>
<p>输入：基于Skip-Gram的语料训练样本，词向量的维度大小$Mcount$，Skip-Gram的上下文大小$2c$,步长$η$，负采样的个数neg。</p>
<p>输出：词汇表每个词对应的模型参数$θ$，所有的词向量$x_w$</p>
<ol>
<li><p>随机初始化所有的模型参数$θ$，所有的词向量$w$</p>
</li>
<li><p>对于每个训练样本$(context(w0),w0)$,负采样出neg个负例中心词$w_i,i=1,2,…neg$</p>
</li>
<li><p>进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w_0),w_0,w_1,…w_neg)$做如下处理：</p>
<p>3.1. for i =1 to 2c:</p>
<p>​    i) e=0</p>
<p>​    ii) for j= 0 to neg, 计算：</p>
<script type="math/tex; mode=display">
f = \sigma(x_{w_{0i}}^T\theta^{w_j})\\
g = (y_j-f)\eta\\
e = e + g\theta^{w_j}\\
\theta^{w_j}= \theta^{w_j} + gx_{w_{0i}}</script><p>​    iii) 词向量更新：</p>
<script type="math/tex; mode=display">
x_{w_{0i}} = x_{w_{0i}} + e</script><p>3.2. 如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤3.1继续迭代。</p>
</li>
</ol>
<h2 id="14、Negative-Sampling的模型源码和算法的对应"><a href="#14、Negative-Sampling的模型源码和算法的对应" class="headerlink" title="14、Negative Sampling的模型源码和算法的对应"></a>14、Negative Sampling的模型源码和算法的对应</h2><p>这里给出上面算法和<a target="_blank" rel="noopener" href="https://github.com/tmikolov/word2vec/blob/master/word2vec.c">word2vec源码</a>中的变量对应关系。</p>
<p>在源代码中，基于Negative Sampling的CBOW模型算法在464-494行，基于Negative Sampling的Skip-Gram的模型算法在520-542行。大家可以对着源代码再深入研究下算法。</p>
<p>在源代码中，neule对应我们上面的$e$, syn0对应我们的$x_w$, $syn1neg$对应我们的$θ_{wi}$, layer1_size对应词向量的维度，window对应我们的$c$。negative对应我们的neg, table_size对应我们负采样中的划分数$M$。</p>
<p>另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。这些和基于Hierarchical Softmax的是一样的。</p>
<p>以上就是基于Negative Sampling的word2vec模型，希望可以帮到大家，后面会讲解用gensim的python版word2vec来使用word2vec解决实际问题。</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">1、词向量基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81CBOW%E4%B8%8ESkip-Gram%E7%94%A8%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">2、CBOW与Skip-Gram用于神经网络语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81word2vec%E5%9F%BA%E7%A1%80%E4%B9%8B%E9%9C%8D%E5%A4%AB%E6%9B%BC%E6%A0%91"><span class="toc-number">3.</span> <span class="toc-text">3、word2vec基础之霍夫曼树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">4.</span> <span class="toc-text">3、基于Hierarchical Softmax的模型概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">5.</span> <span class="toc-text">4、基于Hierarchical Softmax的模型梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84CBOW%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.</span> <span class="toc-text">5、基于Hierarchical Softmax的CBOW模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81%E5%9F%BA%E4%BA%8EHierarchical-Softmax%E7%9A%84Skip-Gram%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.</span> <span class="toc-text">6、基于Hierarchical Softmax的Skip-Gram模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7%E3%80%81Hierarchical-Softmax%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">8.</span> <span class="toc-text">7、Hierarchical Softmax的模型源码和算法的对应</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8%E3%80%81Hierarchical-Softmax%E7%9A%84%E7%BC%BA%E7%82%B9%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">9.</span> <span class="toc-text">8、Hierarchical Softmax的缺点与改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">10.</span> <span class="toc-text">9、基于Negative Sampling的模型概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">11.</span> <span class="toc-text">10、基于Negative Sampling的模型梯度计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11%E3%80%81Negative-Sampling%E8%B4%9F%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">12.</span> <span class="toc-text">11、Negative Sampling负采样方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84CBOW%E6%A8%A1%E5%9E%8B"><span class="toc-number">13.</span> <span class="toc-text">12、基于Negative Sampling的CBOW模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13%E3%80%81%E5%9F%BA%E4%BA%8ENegative-Sampling%E7%9A%84Skip-Gram%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">13、基于Negative Sampling的Skip-Gram模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14%E3%80%81Negative-Sampling%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%BA%90%E7%A0%81%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%BA%94"><span class="toc-number">15.</span> <span class="toc-text">14、Negative Sampling的模型源码和算法的对应</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2021/02/25/word2vec/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2021/02/25/word2vec/&text=word2vec"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2021/02/25/word2vec/&is_video=false&description=word2vec"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=word2vec&body=Check out this article: http://example.com/2021/02/25/word2vec/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2021/02/25/word2vec/&title=word2vec"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2021/02/25/word2vec/&name=word2vec&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2021/02/25/word2vec/&t=word2vec"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2021-2025
    CAKGOD
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
