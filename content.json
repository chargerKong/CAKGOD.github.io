[{"title":"协同过滤","date":"2021-02-26T09:06:44.000Z","path":"2021/02/26/协同过滤/","text":"推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。 推荐算法概述推荐算法是非常古老的，在机器学习还没有兴起的时候就有需求和应用了。概括来说，可以分为以下5种： 1）基于内容的推荐：这一类一般依赖于自然语言处理NLP的一些知识，通过挖掘文本的TF-IDF特征向量，来得到用户的偏好，进而做推荐。这类推荐算法可以找到用户独特的小众喜好，而且还有较好的解释性。这一类由于需要NLP的基础，本文就不多讲，在后面专门讲NLP的时候再讨论。 2）协调过滤推荐：本文后面要专门讲的内容。协调过滤是推荐算法中目前最主流的种类，花样繁多，在工业界已经有了很多广泛的应用。它的优点是不需要太多特定领域的知识，可以通过基于统计的机器学习算法来得到较好的推荐效果。最大的优点是工程上容易实现，可以方便应用到产品中。目前绝大多数实际应用的推荐算法都是协同过滤推荐算法。 3）混合推荐：这个类似我们机器学习中的集成学习，博才众长，通过多个推荐算法的结合，得到一个更好的推荐算法，起到三个臭皮匠顶一个诸葛亮的作用。比如通过建立多个推荐算法的模型，最后用投票法决定最终的推荐结果。混合推荐理论上不会比单一任何一种推荐算法差，但是使用混合推荐，算法复杂度就提高了，在实际应用中有使用，但是并没有单一的协调过滤推荐算法，比如逻辑回归之类的二分类推荐算法广泛。 4）基于规则的推荐：这类算法常见的比如基于最多用户点击，最多用户浏览等，属于大众型的推荐方法，在目前的大数据时代并不主流。 5）基于人口统计信息的推荐：这一类是最简单的推荐算法了，它只是简单的根据系统用户的基本信息发现用户的相关程度，然后进行推荐，目前在大型系统中已经较少使用。 协同过滤推荐概述协同过滤(Collaborative Filtering)作为推荐算法中最经典的类型，包括在线的协同和离线的过滤两部分。所谓在线协同，就是通过在线数据找到用户可能喜欢的物品，而离线过滤，则是过滤掉一些不值得推荐的数据，比比如推荐值评分低的数据，或者虽然推荐值高但是用户已经购买的数据。 协同过滤的模型一般为m个物品，m个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白，此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。 一般来说，协同过滤推荐分为三种类型。第一种是基于用户(user-based)的协同过滤，第二种是基于项目(item-based)的协同过滤，第三种是基于模型(model based)的协同过滤。 基于用户(user-based)的协同过滤主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。而基于项目(item-based)的协同过滤和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户。比如你在网上买了一本机器学习相关的书，网站马上会推荐一堆机器学习，大数据相关的书给你，这里就明显用到了基于项目的协同过滤思想。 我们可以简单比较下基于用户的协同过滤和基于项目的协同过滤：基于用户的协同过滤需要在线找用户和用户之间的相似度关系，计算复杂度肯定会比基于基于项目的协同过滤高。但是可以帮助用户找到新类别的有惊喜的物品。而基于项目的协同过滤，由于考虑的物品的相似性一段时间不会改变，因此可以很容易的离线计算，准确度一般也可以接受，但是推荐的多样性来说，就很难带给用户惊喜了。一般对于小型的推荐系统来说，基于项目的协同过滤肯定是主流。但是如果是大型的推荐系统来说，则可以考虑基于用户的协同过滤，当然更加可以考虑我们的第三种类型，基于模型的协同过滤。 基于模型(model based)的协同过滤是目前最主流的协同过滤类型了，我们的一大堆机器学习算法也可以在这里找到用武之地。下面我们就重点介绍基于模型的协同过滤。 基于模型的协同过滤基于模型的协同过滤作为目前最主流的协同过滤类型，其相关算法可以写一本书了，当然我们这里主要是对其思想做有一个归类概括。我们的问题是这样的m个物品，m个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白，此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。 对于这个问题，用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络,图模型以及隐语义模型来解决。下面我们分别加以介绍。 用关联算法做协同过滤一般我们可以找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。如果用户购买了频繁N项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等。常用的关联推荐算法有Apriori，FP Tree和PrefixSpan。 用聚类算法做协同过滤用聚类算法做协同过滤就和前面的基于用户或者项目的协同过滤有些类似了。我们可以按照用户或者按照物品基于一定的距离度量来进行聚类。如果基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将同样目标人群评分高的物品推荐给目标用户。基于物品聚类的话，则是将用户评分高物品的相似同类物品推荐给用户。常用的聚类推荐算法有K-Means, BIRCH, DBSCAN和谱聚类。 用分类算法做协同过滤如果我们根据用户评分的高低，将分数分成几段的话，则这个问题变成分类问题。比如最直接的，设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们将问题变成了一个二分类问题。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。为啥是逻辑回归而不是看起来更加高大上的比如支持向量机呢？因为逻辑回归的解释性比较强，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。目前逻辑回归做协同过滤在BAT等大厂已经非常成熟了。常见的分类推荐算法有逻辑回归和朴素贝叶斯，两者的特点是解释性很强。 用回归算法做协同过滤用回归算法做协同过滤比分类算法看起来更加的自然。我们的评分可以是一个连续的值而不是离散的值，通过回归模型我们可以得到目标用户对某商品的预测打分。常用的回归推荐算法有Ridge回归，回归树和支持向量回归。 用矩阵分解做协同过滤用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解SVD要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的SVD到协同过滤是比较复杂的。目前主流的矩阵分解推荐算法主要是SVD的一些变种，比如FunkSVD，BiasSVD和SVD++。这些算法和传统SVD的最大区别是不再要求将矩阵分解为$UΣV^T$的形式，而变是两个低秩矩阵$P^TQ$的乘积形式。对于矩阵分解的推荐算法，后续我会专门开篇来讲。 用神经网络做协同过滤用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM)。在目前的Netflix算法比赛中， RBM算法的表现很牛。当然如果用深层的神经网络来做协同过滤应该会更好，大厂商用深度学习的方法来做协同过滤应该是将来的一个趋势。后续我会专门开篇来讲讲RBM。 用图模型做协同过滤用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是SimRank系列算法和马尔科夫模型算法。对于SimRank系列算法，它的基本思想是被相似对象引用的两个对象也具有相似性。算法思想有点类似于大名鼎鼎的PageRank。而马尔科夫模型算法当然是基于马尔科夫链了，它的基本思想是基于传导性来找出普通距离度量算法难以找出的相似性。后续我会专门开篇来讲讲SimRank系列算法。 用隐语义模型做协同过滤隐语义模型主要是基于NLP的，涉及到对用户行为的语义分析来做评分推荐，主要方法有隐性语义分析LSA和隐含狄利克雷分布LDA，这些等讲NLP的再专门讲。 协同过滤的一些新方向当然推荐算法的变革也在进行中，就算是最火爆的基于逻辑回归推荐算法也在面临被取代。哪些算法可能取代逻辑回归之类的传统协同过滤呢？下面是我的理解： a) 基于集成学习的方法和混合推荐:这个和混合推荐也靠在一起了。由于集成学习的成熟，在推荐算法上也有较好的表现。一个可能取代逻辑回归的算法是GBDT。目前GBDT在很多算法比赛都有好的表现，而有工业级的并行化实现类库。 b)基于矩阵分解的方法：矩阵分解，由于方法简单，一直受到青睐。目前开始渐渐流行的矩阵分解方法有分解机(Factorization Machine)和张量分解(Tensor Factorization)。 c) 基于深度学习的方法：目前两层的神经网络RBM都已经有非常好的推荐算法效果，而随着深度学习和多层神经网络的兴起，以后可能推荐算法就是深度学习的天下了？目前看最火爆的是基于CNN和RNN的推荐算法。 协同过滤总结协同过滤作为一种经典的推荐算法种类，在工业界应用广泛，它的优点很多，模型通用性强，不需要太多对应数据领域的专业知识，工程实现简单，效果也不错。这些都是它流行的原因。 当然，协同过滤也有些难以避免的难题，比如令人头疼的“冷启动”问题，我们没有新用户任何数据的时候，无法较好的为新用户推荐物品。同时也没有考虑情景的差异，比如根据用户所在的场景和用户当前的情绪。当然，也无法得到一些小众的独特喜好，这块是基于内容的推荐比较擅长的。"},{"title":"Aprior算法","date":"2021-02-26T08:22:17.000Z","path":"2021/02/26/Aprior算法/","text":"Apriori算法是常用的用于挖掘出数据关联规则的算法，它用来找出数据值中频繁出现的数据集合，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置，达到节约成本，增加经济效益的目的。下面我们就对Apriori算法做一个总结。 频繁项集的评估标准什么样的数据才是频繁项集呢？也许你会说，这还不简单，肉眼一扫，一起出现次数多的数据集就是频繁项集吗！的确，这也没有说错，但是有两个问题，第一是当数据量非常大的时候，我们没法直接肉眼发现频繁项集，这催生了关联规则挖掘的算法，比如Apriori, PrefixSpan, CBA。第二是我们缺乏一个频繁项集的标准。比如10条记录，里面A和B同时出现了三次，那么我们能不能说A和B一起构成频繁项集呢？因此我们需要一个评估频繁项集的标准。 常用的频繁项集的评估标准有支持度,置信度和提升度三个。 支持度就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。如果我们有两个想分析关联性的数据X和Y，则对应的支持度为: Support(X,Y) = P(XY) = \\frac{number(XY)}{num(All Samples)}以此类推，如果我们有三个想分析关联性的数据X，Y和Z，则对应的支持度为: Support(X,Y,Z) = P(XYZ) = \\frac{number(XYZ)}{num(All Samples)}一般来说，支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。 置信度体现了一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。如果我们有两个想分析关联性的数据X和Y，X对Y的置信度为 Confidence(X \\Leftarrow Y) = P(X|Y)=P(XY)/P(Y)也可以以此类推到多个数据的关联置信度，比如对于三个数据X，Y，Z,则X对于Y和Z的置信度为： Confidence(X \\Leftarrow YZ) = P(X|YZ)=P(XYZ)/P(YZ)举个例子，在购物数据中，纸巾对应鸡爪的置信度为40%，支持度为1%。则意味着在购物数据中，总共有1%的用户既买鸡爪又买纸巾;同时买鸡爪的用户中有40%的用户购买纸巾。 提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比，即: Lift(X \\Leftarrow Y) = P(X|Y)/P(X) = Confidence(X \\Leftarrow Y) / P(X)提升度体先了X和Y之间的关联关系, 提升度大于1则$X⇐Y$是有效的强关联规则， 提升度小于等于1则$X⇐Y$是无效的强关联规则 。一个特殊的情况，如果X和Y独立，则有$Lift(X⇐Y)=1$，因为此时$P(X|Y)=P(X)$。 一般来说，要选择一个数据集合中的频繁数据集，则需要自定义评估标准。最常用的评估标准是用自定义的支持度，或者是自定义支持度和置信度的一个组合。 Apriori算法思想对于Apriori算法，我们使用支持度来作为我们判断频繁项集的标准。Apriori算法的目标是找到最大的K项频繁集。这里有两层意思，首先，我们要找到符合支持度标准的频繁集。但是这样的频繁集可能有很多。第二层意思就是我们要找到最大个数的频繁集。比如我们找到符合支持度的频繁集AB和ABE，那么我们会抛弃AB，只保留ABE，因为AB是2项频繁集，而ABE是3项频繁集。那么具体的，Apriori算法是如何做到挖掘K项频繁集的呢？ Apriori算法采用了迭代的方法，先搜索出候选1项集及对应的支持度，剪枝去掉低于支持度的1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选的频繁2项集，筛选去掉低于支持度的候选频繁2项集，得到真正的频繁二项集，以此类推，迭代下去，直到无法找到频繁k+1项集为止，对应的频繁k项集的集合即为算法的输出结果。 可见这个算法还是很简洁的，第i次的迭代过程包括扫描计算候选频繁i项集的支持度，剪枝得到真正频繁i项集和连接生成候选频繁i+1项集三步。 我们下面这个简单的例子看看： 我们的数据集D有4条记录，分别是134,235,1235和25。现在我们用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先我们生成候选频繁1项集，包括我们所有的5个数据并计算5个数据的支持度，计算完毕后我们进行剪枝，数据4由于支持度只有25%被剪掉。我们最终的频繁1项集为1235，现在我们链接生成候选频繁2项集，包括12,13,15,23,25,35共6组。此时我们的第一轮迭代结束。 进入第二轮迭代，我们扫描数据集计算候选频繁2项集的支持度，接着进行剪枝，由于12和15的支持度只有25%而被筛除，得到真正的频繁2项集，包括13,23,25,35。现在我们链接生成候选频繁3项集,123, 135和235共3组，这部分图中没有画出。通过计算候选频繁3项集的支持度，我们发现123和135的支持度均为25%，因此接着被剪枝，最终得到的真正频繁3项集为235一组。由于此时我们无法再进行数据连接，进而得到候选频繁4项集，最终的结果即为频繁3三项集235。 Aprior算法流程下面我们对Aprior算法流程做一个总结。 输入：数据集合D，支持度阈值αα 输出：最大的频繁k项集 1）扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。k=1，频繁0项集为空集。 2）挖掘频繁k项集 ​ a) 扫描数据计算候选频繁k项集的支持度 ​ b) 去除候选频繁k项集中支持度低于阈值的数据集,得到频繁k项集。如果得到的频繁k项集为空，则直接返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。 ​ c) 基于频繁k项集，连接生成候选频繁k+1项集。 3） 令k=k+1，转入步骤2。 从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。 总结Aprior算法是一个非常经典的频繁项集的挖掘算法，很多算法都是基于Aprior算法而产生的，包括FP-Tree,GSP, CBA等。这些算法利用了Aprior算法的思想，但是对算法做了改进，数据挖掘效率更好一些，因此现在一般很少直接用Aprior算法来挖掘数据了，但是理解Aprior算法是理解其它Aprior类算法的前提，同时算法本身也不复杂，因此值得好好研究一番。 不过scikit-learn中并没有频繁集挖掘相关的算法类库，这不得不说是一个遗憾，不知道后面的版本会不会加上。"},{"title":"典型关联分析（CCA）","date":"2021-02-26T07:06:27.000Z","path":"2021/02/26/典型关联分析（CCA）/","text":"CCA概述在数理统计里面，我们都知道相关系数这个概念。假设有两组一维的数据集X和Y，则相关系数$ρ$的定义为: \\rho(X,Y) = \\frac{cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}其中$cov(X,Y)$是X和Y的协方差，而$D(X),D(Y)$分别是X和Y的方差。相关系数$ρ$的取值为[-1,1], $ρ$的绝对值越接近于1，则X和Y的线性相关性越高。越接近于0，则X和Y的线性相关性越低。 虽然相关系数可以很好的帮我们分析一维数据的相关性，但是对于高维数据就不能直接使用了。拿上面我们提到的，如果X是包括人身高和体重两个维度的数据，而Y是包括跑步能力和跳远能力两个维度的数据，就不能直接使用相关系数的方法。那我们能不能变通一下呢？CCA给了我们变通的方法。 CCA使用的方法是将多维的X和Y都用线性变换为1维的X’和Y’，然后再使用相关系数来看X’和Y’的相关性。将数据从多维变到1位，也可以理解为CCA是在进行降维，将高维数据降到1维，然后再用相关系数进行相关性的分析。下面我们看看CCA的算法思想。 算法思想上面我们提到CCA是将高维的两组数据分别降维到1维，然后用相关系数分析相关性。但是有一个问题是，降维的标准是如何选择的呢？回想下主成分分析PCA，降维的原则是投影方差最大；再回想下线性判别分析LDA，降维的原则是同类的投影方差小，异类间的投影方差大。对于我们的CCA，它选择的投影标准是降维到1维后，两组数据的相关系数最大。 现在我们具体来讨论下CCA的算法思想。假设我们的数据集是X和Y，X为$n1×m$的样本矩阵。Y为$n2×m$的样本矩阵.其中m为样本个数，而$n1,n2$分别为X和Y的特征维度。 对于X矩阵，我们将其投影到1维，或者说进行线性表示，对应的投影向量或者说线性系数向量为$a$, 对于Y矩阵，我们将其投影到1维，或者说进行线性表示，对应的投影向量或者说线性系数向量为$b$, 这样X ,Y投影后得到的一维向量分别为X’,Y’。我们有: X' = a^TX, Y'=b^TYCCA的优化目标是最大化$ρ(X′,Y′)$得到对应的投影向量$a,b$，即: \\underbrace{arg\\;max}_{a,b}\\frac{cov(X',Y')}{\\sqrt{D(X')}\\sqrt{D(Y')}}在投影前，我们一般会把原始数据进行标准化，得到均值为0而方差为1的数据X和Y。这样我们有： cov(X',Y') = cov(a^TX, b^TY) = E() = E((a^TX)(b^TY)^T) = a^TE(XY^T)b\\\\ D(X') = D(a^TX) = a^TE(XX^T)a\\\\ D(Y') = D(b^TY) = b^TE(YY^T)b由于我们的X，Y的均值均为0，则 D(X) = cov(X,X) = E(XX^T), D(Y)= cov(Y,Y) = E(YY^T)\\\\ cov(X,Y) = E(XY^T), cov(Y,X) = E(YX^T)令$S_{XY}=cov(X,Y)$，则优化目标可以转化为： \\underbrace{arg\\;max}_{a,b}\\frac{a^TS_{XY}b}{\\sqrt{ a^TS_{XX}a}\\sqrt{b^TS_{YY}b}}由于分子分母增大相同的倍数，优化目标结果不变，我们可以采用和SVM类似的优化方法，固定分母，优化分子，具体的转化为： \\underbrace{arg\\;max}_{a,b}\\;\\;{a^TS_{XY}b}\\\\ s.t. a^TS_{XX}a =1,\\; b^TS_{YY}b =1也就是说，我们的CCA算法的目标最终转化为一个凸优化过程，只要我们求出了这个优化目标的最大值，就是我们前面提到的多维X和Y的相关性度量，而对应的$a,b$则为降维时的投影向量，或者说线性系数。 这个函数优化一般有两种方法，第一种是奇异值分解SVD，第二种是特征分解，两者得到的结果一样，下面我们分别讲解。 CCA算法的SVD求解对于上面的优化目标，我们可以做一次矩阵标准化，就可以用SVD来求解了。 首先，我们令$a=S_{XX}^{-1/2}u, b=S_{YY}^{-1/2}v$,这样我们有： a^TS_{XX}a =1 \\Rightarrow u^TS_{XX}^{-1/2}S_{XX}S_{XX}^{-1/2}u =1 \\Rightarrow u^Tu=1\\\\ b^TS_{YY}b =1 \\Rightarrow v^TS_{YY}^{-1/2}S_{YY}S_{YY}^{-1/2}v=1 \\Rightarrow v^Tv=1\\\\ a^TS_{XY}b = u^TS_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}v也就是说，我们的优化目标变成下式： \\underbrace{arg\\;max}_{u,v}u^TS_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}v\\\\s.t. u^Tu =1,\\; v^Tv =1仔细一看，如果将u和v看做矩阵$M=S_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}$的某一个奇异值对应的左右奇异向量。那么利用奇异值分解，我们可以得到$M={UΣV}^T$,其中$U,V$分别为M的左奇异向量和右奇异向量组成的矩阵，而$Σ$为M的奇异值组成的对角矩阵。由于$U,V$所有的列都为标准正交基，则$u^TU$和$V^Tv$得到一个只有一个标量值为1，其余标量值为0的向量。此时我们有 u^TS_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}v = u^TU\\Sigma V^Tv = \\sigma_{uv}也就是说我们最大化$u^TS_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}v$,其实对应的最大值就是某一组左右奇异向量所对应的奇异值的最大值。也就是将M做了奇异值分解后，最大的奇异值就是我们优化目标的最大值，或者说我们的X和Y之间的最大相关系数。利用对应的左右奇异向量$u,v$我们也可以求出我们原始的X和Y的线性系数$a=S_{XX}^{-1/2}u, b=S_{YY}^{-1/2}v$。 CCA算法的特征分解求解特征分解方式就比较传统了，利用拉格朗日函数，优化目标转化为最大化下式： J(a,b) = a^TS_{XY}b -\\frac{\\lambda}{2}(a^TS_{XX}a-1)-\\frac{\\theta}{2}(b^TS_{YY}b-1)分别对$a,b$求导并令结果为0，我们得到： S_{XY}b-\\lambda S_{XX}a=0\\\\S_{YX}a-\\theta S_{YY}b=0将上面第一个式子左乘$a^T$,第二个式子左乘$b^T$，并利用$a^TS_{XX}a =1,\\; b^TS_{YY}b =1$，我们得到 \\lambda = \\theta = a^TS_{XY}b其实也就是说我们的拉格朗日系数就是我们要优化的目标。我们继续将上面的两个式子做整理，第一个式子左乘$S_{XX}^{-1}$,第二个式子左乘$S_{YY}^{-1}$，我们得到： S_{XX}^{-1}S_{XY}b=\\lambda a\\\\S_{YY}^{-1}S_{YX}a = \\lambda b将上面第二个式子带入第一个式子，我们得到: S_{XX}^{-1}S_{XY}S_{YY}^{-1}S_{YX}a=\\lambda^2a这个式子我们就熟悉了，这不就是特征分解吗！要求最大的相关系数$λ$,我们只需要对矩阵$N’=S_{YY}^{-1}S_{YX}S_{XX}^{-1}S_{XY}$,做特征分解，找出最大的特征值取平方根即可，此时最大特征值对应的特征向量即为Y的线性系数$b$。 可以看出特征分解的方法要比SVD复杂，但是两者求得的结果其实是等价的，只要利用SVD和特征分解之间的关系就很容易发现两者最后的结果相同。 CCA算法流程这里我们对CCA的算法流程做一个总结，以SVD方法为准。 输入：各为m个的样本X和Y，X和Y的维度都大于1 输出：X,Y的相关系数$ρ$,X和Y的线性系数向量a和b 1）计算X的方差$S_{XX}$, Y的方差$S_{YY}$，X和Y的协方差$S_{XY}$,Y和X的协方差$S_{YX}=S_{XY}^T$, 2) 计算矩阵$M=S_{XX}^{-1/2}S_{XY}S_{YY}^{-1/2}$ 3) 对矩阵$M$进行奇异值分解，得到最大的奇异值$ρ$，和最大奇异值对应的左右奇异向量$u,v$, 4) 计算X和Y的线性系数向量a和b,$a=S_{XX}^{-1/2}u, b=S_{YY}^{-1/2}v$ 总结CCA算法广泛的应用于数据相关度的分析，同时还是偏最小二乘法的基础。但是由于它依赖于数据的线性表示，当我们的数据无法线性表示时，CCA就无法使用，此时我们可以利用核函数的思想，将数据映射到高维后，再利用CCA的思想降维到1维，求对应的相关系数和线性关系，这个算法一般称为KCCA。 此外，我们在算法里只找了相关度最大的奇异值或者特征值，作为数据的相关系数，实际上我们也可以像PCA一样找出第二大奇异值，第三大奇异值，。。。得到第二相关系数和第三相关系数。然后对数据做进一步的相关性分析。但是一般的应用来说，找出第一相关系数就可以了。 有时候我们的矩阵$S_{XX},S_{YY}$不可逆，此时我们得不到对应的逆矩阵，一般遇到这种情况可以对$S_{XX},S_{YY}$进行正则化，将$S_{XX},S_{YY}$变化为$S_{XX}+\\gamma I,S_{YY}+\\gamma I$然后继续求逆。其中$γ$为正则化系数。"},{"title":"文本主题模型","date":"2021-02-25T08:59:00.000Z","path":"2021/02/25/文本主题模型/","text":"本文介绍三种文本主题模型，包括潜在语义索引（LSI）、非负矩阵分解（NMF）和LDA，参考刘建平的博客。 在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。 聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。 那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。 当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。 潜在语义索引（LSI）潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。 这里我们简要回顾下SVD：对于一个$m×n$的矩阵$A$，可以分解为下面三个矩阵： A_{m \\times n} = U_{m \\times m}\\Sigma_{m \\times n} V^T_{n \\times n}有时为了降低矩阵的维度到k，SVD的分解可以近似的写为： A_{m \\times n} \\approx U_{m \\times k}\\Sigma_{k \\times k} V^T_{k \\times n}如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$应第j个词和第m个词义的相关度。$Σ_{lm}$对应第l个主题和第m个词义的相关度。 也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$Σ_{lm}$对应第l个词义和第m个主题的相关度。 这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。 下面介绍一个简单的例子，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下： 这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为： 从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。 在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ： sim(d1,d2) = \\frac{(-0.4945)*(-0.6458) + (0.6492)*(-0.7194)}{\\sqrt{(-0.4945)^2+0.6492^2}\\sqrt{(-0.6458)^2+(-0.7194)^2}}对LSI模型进行总结，LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。 1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。 2） 主题值的选取对结果的影响非常大，很难选择合适的k值。 3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。 对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。 回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。 非负矩阵分解（NMF）前面讲到了LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？ 非负矩阵分解(non-negative matrix factorization，以下简称NMF)是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识别，语音识别等，这里我们会主要关注于它在文本主题模型里的运用。 回顾奇异值分解，它会将一个矩阵分解为三个矩阵： A = U\\Sigma V^T如果降维到$k$维，则表达式为： A_{m \\times n} \\approx U_{m \\times k}\\Sigma_{k \\times k} V^T_{k \\times n}但是NMF虽然也是矩阵分解，它却使用了不同的思路，它的目标是期望将矩阵分解为两个矩阵: A_{m \\times n} \\approx W_{m \\times k}H_{k \\times n}下面讲解NMF的优化思路，NMF期望找到这样的两个矩阵$W,H$，使$WH$的矩阵乘积得到的矩阵对应的每个位置的值和原矩阵$A$对应位置的值相比误差尽可能的小。用数学的语言表示就是： \\underbrace{arg\\;min}_{W,H}\\frac{1}{2}\\sum\\limits_{i,j}(A_{ij}-(WH)_{ij})^2如果完全用矩阵表示，则为： \\underbrace{arg\\;min}_{W,H}\\frac{1}{2}||A-WH||_{Fro}^2其中，$||∗||_{Fro}$为Frobenius范数。 当然对于这个式子，我们也可以加上L1和L2的正则化项如下： \\underbrace{arg\\;min}_{W,H}\\frac{1}{2}||A-WH||_{Fro}^2 +\\alpha\\rho|| W||_1+\\alpha\\rho|| H||_1+\\frac{\\alpha(1-\\rho)}{2}|| W||_{Fro}^2 + \\frac{\\alpha(1-\\rho)}{2}|| H||_{Fro}^2其中，$α$为L1&amp;L2正则化参数，而$ρ$为L1正则化占总正则化项的比例。$||∗||_1$为L1范数。 我们要求解的有$m∗k+k∗n$个参数。参数不少，常用的迭代方法有梯度下降法和拟牛顿法。不过如果我们决定加上了L1正则化的话就不能用梯度下降和拟牛顿法了。此时可以用坐标轴下降法或者最小角回归法来求解。scikit-learn中NMF的库目前是使用坐标轴下降法来求解的，即在迭代时，一次固定$m∗k+k∗n−1$个参数，仅仅最优化一个参数。 NMF矩阵分解如何运用到我们的主题模型呢？ 此时NMF可以这样解释：我们输入的有m个文本，n个词，而$A_{ij}$对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个文本的和第k个主题的概率相关度，而$H_{kj}$对应第j个词和第k个主题的概率相关度。 当然也可以反过来去解释：我们输入的有m个词，n个文本，而$A_{ij}$对应第i个词的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ik}$对应第i个词的和第k个主题的概率相关度，而$H_{kj}$对应第j个文本和第k个主题的概率相关度。 注意到这里我们使用的是”概率相关度”，这是因为我们使用的是”非负”的矩阵分解，这样我们的$W,H$矩阵值的大小可以用概率值的角度去看。从而可以得到文本和主题的概率分布关系。第二种解释用一个图来表示如下： 和LSI相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。当然NMF由于是两个矩阵，相比LSI的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价。 在 scikit-learn中，NMF在sklearn.decomposition.NMF包中，它支持L1和L2的正则化，而$W,H$的求解使用坐标轴下降法来实现。示例代码如下： import numpy as np X = np.array([[1,1,5,2,3], [0,6,2,1,1], [3, 4,0,3,1], [4, 1,5,6,3]]) from sklearn.decomposition import NMF model = NMF(n_components=2, alpha=0.01) W = model.fit_transform(X) H = model.components_ 对NMF模型进行总结，NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本识别不一定很准确。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。 LDA开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet Allocation，以下简称LDA)。注意机器学习还有一个LDA，即线性判别分析，主要是用于降维和分类的，如果大家需要了解这个LDA的信息，参看之前写的线性判别分析LDA原理总结。文本关注于隐含狄利克雷分布对应的LDA。 LDA贝叶斯模型LDA是基于贝叶斯模型的，涉及到贝叶斯模型离不开“先验分布”，“数据（似然）”和”后验分布”三块。在朴素贝叶斯算法原理小结中我们也已经讲到了这套贝叶斯理论。在贝叶斯学派这里： 先验分布 + 数据（似然）= 后验分布这点其实很好理解，因为这符合我们人的思维方式，比如你对好人和坏人的认知，先验分布为：100个好人和100个的坏人，即你认为好人坏人各占一半，现在你被2个好人（数据）帮助了和1个坏人骗了，于是你得到了新的后验分布为：102个好人和101个的坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成你的新的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布为：103个好人和104个的坏人。依次继续更新下去。 二项分布与Beta分布贝叶斯模型和认知过程，假如用数学和概率的方式该如何表达呢？对于我们的数据（似然），这个好办，用一个二项分布就可以搞定，即对于二项分布： Binom(k|n,p) = {n \\choose k}p^k(1-p)^{n-k}其中p我们可以理解为好人的概率，k为好人的个数，n为好人坏人的总数。虽然数据(似然)很好理解，但是对于先验分布，我们就要费一番脑筋了，为什么呢？因为我们希望这个先验分布和数据（似然）对应的二项分布集合后，得到的后验分布在后面还可以作为先验分布！就像上面例子里的“102个好人和101个的坏人”，它是前面一次贝叶斯推荐的后验分布，又是后一次贝叶斯推荐的先验分布。也即是说，我们希望先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫共轭分布。在我们的例子里，我们希望找到和二项分布共轭的分布。和二项分布共轭的分布其实就是Beta分布。Beta分布的表达式为： Beta(p|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^NaN其中$Γ$是Gamma函数，满足$Γ(x)=(x−1)!$ 仔细观察Beta分布和二项分布，可以发现两者的密度函数很相似，区别仅仅在前面的归一化的阶乘项。那么它如何做到先验分布和后验分布的形式一样呢？后验分布$P(p|n,k,α,β)$推导如下： \\begin{align} P(p|n,k,\\alpha,\\beta) & \\propto P(k|n,p)P(p|\\alpha,\\beta) \\\\ & = P(k|n,p)P(p|\\alpha,\\beta) \\\\& = Binom(k|n,p) Beta(p|\\alpha,\\beta) \\\\ &= {n \\choose k}p^k(1-p)^{n-k} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^NaN \\\\& \\propto p^{k+\\alpha-1}(1-p)^{n-k + \\beta -1} \\end{align}将上面最后的式子归一化以后，得到我们的后验概率为： P(p|n,k,\\alpha,\\beta) = \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + k)\\Gamma(\\beta + n - k)}p^{k+\\alpha-1}(1-p)^{n-k + \\beta -1}可见我们的后验分布的确是Beta分布，而且我们发现： Beta(p|\\alpha,\\beta) + BinomCount(k,n-k) = Beta(p|\\alpha + k,\\beta +n-k)这个式子完全符合我们在上一节好人坏人例子里的情况，我们的认知会把数据里的好人坏人数分别加到我们的先验分布上，得到后验分布。 我们在来看看Beta分布$Beta(p|α,β)$的期望: \\begin{align} E(Beta(p|\\alpha,\\beta)) & = \\int_{0}^{1}tBeta(p|\\alpha,\\beta)dt \\\\& = \\int_{0}^{1}t \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}t^{\\alpha-1}(1-t)^NaNdt \\\\& = \\int_{0}^{1}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}t^{\\alpha}(1-t)^NaNdt \\end{align}由于上式最右边的乘积对应Beta分布$Beta(p|α+1,β)$,因此有： \\int_{0}^{1}\\frac{\\Gamma(\\alpha + \\beta+1)}{\\Gamma(\\alpha+1)\\Gamma(\\beta)}p^{\\alpha}(1-p)^NaN dp=1这样我们的期望可以表达为： E(Beta(p|\\alpha,\\beta)) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(\\alpha+1)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta+1)} = \\frac{\\alpha}{\\alpha + \\beta}多项分布与Dirichlet 分布现在我们回到上面好人坏人的问题，假如我们发现有第三类人，不好不坏的人，这时候我们如何用贝叶斯来表达这个模型分布呢？之前我们是二维分布，现在是三维分布。由于二维我们使用了Beta分布和二项分布来表达这个模型，则在三维时，以此类推，我们可以用三维的Beta分布来表达先验后验分布，三项的多项分布来表达数据（似然）。 三项的多项分布好表达，我们假设数据中的第一类有$m1$个好人，第二类有$m2$个坏人，第三类为$m3=n−m1−m2$个不好不坏的人,对应的概率分别为$p1,p2,p3=1−p1−p2$，则对应的多项分布为： multi(m_1,m_2,m_3|n,p_1,p_2,p_3) = \\frac{n!}{m_1! m_2!m_3!}p_1^{m_1}p_2^{m_2}p_3^{m_3}那三维的Beta分布呢？超过二维的Beta分布我们一般称之为狄利克雷(以下称为Dirichlet )分布。也可以说Beta分布是Dirichlet 分布在二维时的特殊形式。从二维的Beta分布表达式，我们很容易写出三维的Dirichlet分布如下： Dirichlet(p_1,p_2,p_3|\\alpha_1,\\alpha_2, \\alpha_3) = \\frac{\\Gamma(\\alpha_1+ \\alpha_2 + \\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}p_1^{\\alpha_1-1}(p_2)^{\\alpha_2-1}(p_3)^{\\alpha_3-1}同样的方法，我们可以写出4维，5维，。。。以及更高维的Dirichlet 分布的概率密度函数。为了简化表达式，我们用向量来表示概率和计数,这样多项分布可以表示为：$Dirichlet(p⃗ |α⃗ )$,而多项分布可以表示为：$multi(m⃗ |n,p⃗ )$。 一般意义上的K维Dirichlet 分布表达式为： Dirichlet(\\vec p| \\vec \\alpha) = \\frac{\\Gamma(\\sum\\limits_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)}\\prod_{k=1}^Kp_k^{\\alpha_k-1}而多项分布和Dirichlet 分布也满足共轭关系，这样我们可以得到和上一节类似的结论： Dirichlet(\\vec p|\\vec \\alpha) + MultiCount(\\vec m) = Dirichlet(\\vec p|\\vec \\alpha + \\vec m)对于Dirichlet 分布的期望，也有和Beta分布类似的性质： E(Dirichlet(\\vec p|\\vec \\alpha)) = (\\frac{\\alpha_1}{\\sum\\limits_{k=1}^K\\alpha_k}, \\frac{\\alpha_2}{\\sum\\limits_{k=1}^K\\alpha_k},...,\\frac{\\alpha_K}{\\sum\\limits_{k=1}^K\\alpha_k})LDA主题模型我们的问题是这样的，我们有MM篇文档，对应第d个文档中有有NdNd个词。即输入为如下图： 我们的目标是找到每一篇文档的主题分布和每一个主题中词的分布。在LDA模型中，我们需要先假定一个主题数目$K$，这样所有的分布就都基于$K$个主题展开。那么具体LDA模型是怎么样的呢？具体如下图： LDA假设文档主题的先验分布是Dirichlet分布，即对于任一文档$d$, 其主题分布$θd$为： \\theta_d = Dirichlet(\\vec \\alpha)其中，$α$为分布的超参数，是一个$K$维向量。 LDA假设主题中词的先验分布是Dirichlet分布，即对于任一主题$k$, 其词分布$β_k$为： \\beta_k= Dirichlet(\\vec \\eta)其中，$η$为分布的超参数，是一个$V$维向量。$V$代表词汇表里所有词的个数。 对于数据中任一一篇文档$d$中的第$n$个词，我们可以从主题分布$θ_d$中得到它的主题编号$z_{dn}$的分布为： z_{dn} = multi(\\theta_d)而对于该主题编号，得到我们看到的词$w_{dn}$的概率分布为： w_{dn} = multi(\\beta_{z_{dn}})理解LDA主题模型的主要任务就是理解上面的这个模型。这个模型里，我们有$M$个文档主题的Dirichlet分布，而对应的数据有$M$个主题编号的多项分布，这样$(α→θ_d→{z⃗}_d)$就组成了Dirichlet-multi共轭，可以使用前面提到的贝叶斯推断的方法得到基于Dirichlet分布的文档主题后验分布。 如果在第d个文档中，第k个主题的词的个数为：$n_d^{(k)}$, 则对应的多项分布的计数可以表示为: \\vec n_d = (n_d^{(1)}, n_d^{(2)},...n_d^{(K)})利用Dirichlet-multi共轭，得到$θ_d$的后验分布为： Dirichlet(\\theta_d | \\vec \\alpha + \\vec n_d)同样的道理，对于主题与词的分布，我们有$K$个主题与词的Dirichlet分布，而对应的数据有$K$个主题编号的多项分布，这样$(η→β_k→w⃗_{(k)}$就组成了Dirichlet-multi共轭，可以使用前面提到的贝叶斯推断的方法得到基于Dirichlet分布的主题词的后验分布。 如果在第k个主题中，第v个词的个数为：$n^{(v)}_k$, 则对应的多项分布的计数可以表示为: \\vec n_k = (n_k^{(1)}, n_k^{(2)},...n_k^{(V)})利用Dirichlet-multi共轭，得到$β_k$的后验分布为： Dirichlet(\\beta_k | \\vec \\eta+ \\vec n_k)由于主题产生词不依赖具体某一个文档，因此文档主题分布和主题词分布是独立的。理解了上面这$M+K$组Dirichlet-multi共轭，就理解了LDA的基本原理了。 现在的问题是，基于这个LDA模型如何求解我们想要的每一篇文档的主题分布和每一个主题中词的分布呢？一般有两种方法，第一种是基于Gibbs采样算法求解，第二种是基于变分推断EM算法求解。 LDA求解之Gibbs采样算法LDA求解之变分推断EM算法"},{"title":"word2vec","date":"2021-02-25T02:50:22.000Z","path":"2021/02/25/word2vec/","text":"转载自cnblogs.com/pinard/p/7160330.html word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。 1、词向量基础用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation. One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？ Distributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 有了用Distributed Representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现： \\vec {King} - \\vec {Man} + \\vec {Woman} = \\vec {Queen} 可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2、CBOW与Skip-Gram用于神经网络语言模型在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。 CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。 这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。 Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词”Learning”是我们的输入，而这8个上下文词是我们的输出。 这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。 以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。 word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？ 3、word2vec基础之霍夫曼树word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。 具体如何用霍夫曼树来进行CBOW和Skip-Gram的训练我们在下一节讲，这里我们先复习下霍夫曼树。 霍夫曼树的建立其实并不难，过程如下： 输入：权值为(w1,w2,…wn)(w1,w2,…wn)的nn个节点 输出：对应的霍夫曼树 1）将(w1,w2,…wn)(w1,w2,…wn)看做是有nn棵树的森林，每个树仅有一个节点。 2）在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。 3） 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。 4）重复步骤2）和3）直到森林里只有一棵树为止。 下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a,b,c,d,e,f)共6个节点，节点的权值分布是(20,4,8,6,16,3)。 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是20,8,6,16,7。此时根节点权重最小的6,7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。 那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。 在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。 现在我们开始关注word2vec的语言模型如何改进传统的神经网络的方法。由于word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。 3、基于Hierarchical Softmax的模型概述我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中VV是词汇表的大小， word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：$(1,2,3,4),(9,6,11,8),(5,10,7,12)$,那么我们word2vec映射后的词向量就是$(5,6,7,8)$。由于这里是从多个词向量变成了一个词向量。 第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。 由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词$w2$。 如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即： P(+) = \\sigma(x_w^T\\theta) = \\frac{1}{1+e^{-x_w^T\\theta}}其中$x_w$是当前内部节点的词向量，而$θ$则是我们需要从训练样本求出的逻辑回归的模型参数。 使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为$V$现在变成了$log_2V$。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。 容易理解，被划分为左子树而成为负类的概率为$P(-) = 1-P(+)$。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看$P(−),P(+)$谁的概率值大。而控制$P(−),P(+)$谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$θ$。 对于上图中的$w_2$，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点$n(w_2,1)$的$p(-)$概率大，$n(w_2,2)$的$p(-)$概率大，$n(w_2,3)$的$p(+)$概率大。 回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$θ$, 使训练样本达到最大似然。那么如何达到最大似然呢？ 4、基于Hierarchical Softmax的模型梯度计算我们使用最大似然法来寻找所有节点的词向量和所有内部节点$θ$。先拿上面的$w2$例子来看，我们期望最大化下面的似然函数： \\prod_{i=1}^3P(n(w_i),i) = (1- \\frac{1}{1+e^{-x_w^T\\theta_1}})(1- \\frac{1}{1+e^{-x_w^T\\theta_2}})\\frac{1}{1+e^{-x_w^T\\theta_3}}对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。 为了便于我们后面一般化的描述，我们定义输入的词为$w$,其从输入层词向量求和平均后的霍夫曼树根节点词向量为$x_w$, 从根节点到$w$所在的叶子节点，包含的节点总数为$l_w$, $w$在霍夫曼树中从根节点开始，经过的第$i$个节点表示为$p_i^w$,对应的霍夫曼编码为$d_i^w \\in \\{0,1\\}$,其中$i=2,3,…l_w$。而该节点对应的模型参数表示为$\\theta_i^w$, 其中$i=1,2,…l_w−1$，没有$i=l_w$是因为模型参数仅仅针对于霍夫曼树的内部节点。 定义$w$经过的霍夫曼树某一个节点$j$的逻辑回归概率为$P(d_j^w|x_w, \\theta_{j-1}^w)$，其表达式为： P(d_j^w|x_w, \\theta_{j-1}^w)= \\begin{cases} \\sigma(x_w^T\\theta_{j-1}^w)& {d_j^w=0}\\\\ 1- \\sigma(x_w^T\\theta_{j-1}^w) & {d_j^w = 1} \\end{cases}那么对于某一个目标输出词$w$,其最大似然为： \\prod_{j=2}^{l_w}P(d_j^w|x_w, \\theta_{j-1}^w) = \\prod_{j=2}^{l_w} [\\sigma(x_w^T\\theta_{j-1}^w)] ^{1-d_j^w}[1-\\sigma(x_w^T\\theta_{j-1}^w)]^{d_j^w}在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到$w$的对数似然函数$L$如下： L= log \\prod_{j=2}^{l_w}P(d_j^w|x_w, \\theta_{j-1}^w) = \\sum\\limits_{j=2}^{l_w} ((1-d_j^w) log [\\sigma(x_w^T\\theta_{j-1}^w)] + d_j^w log[1-\\sigma(x_w^T\\theta_{j-1}^w)])要得到模型中$w$词向量和内部节点的模型参数$θ$, 我们使用梯度上升法即可。首先我们求模型参数$\\theta_{j-1}^w$的梯度： \\begin{align} \\frac{\\partial L}{\\partial \\theta_{j-1}^w} & = (1-d_j^w)\\frac{(\\sigma(x_w^T\\theta_{j-1}^w)(1-\\sigma(x_w^T\\theta_{j-1}^w)}{\\sigma(x_w^T\\theta_{j-1}^w)}x_w - d_j^w \\frac{(\\sigma(x_w^T\\theta_{j-1}^w)(1-\\sigma(x_w^T\\theta_{j-1}^w)}{1- \\sigma(x_w^T\\theta_{j-1}^w)}x_w \\\\ & = (1-d_j^w)(1-\\sigma(x_w^T\\theta_{j-1}^w))x_w - d_j^w\\sigma(x_w^T\\theta_{j-1}^w)x_w \\\\& = (1-d_j^w-\\sigma(x_w^T\\theta_{j-1}^w))x_w \\end{align}同样的方法，可以求出$x_w$的梯度表达式如下： \\frac{\\partial L}{\\partial x_w} = \\sum\\limits_{j=2}^{l_w}(1-d_j^w-\\sigma(x_w^T\\theta_{j-1}^w))\\theta_{j-1}^w有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的$\\theta_{j-1}^w$和$x_w$。 5、基于Hierarchical Softmax的CBOW模型由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。 首先我们要定义词向量的维度大小$M$，以及CBOW的上下文大小$2c$,这样我们对于训练样本中的每一个词，其前面的$c$个词和后面的$c$个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。 在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。 对于从输入层到隐藏层（投影层），这一步比较简单，就是对$w$周围的$2c$个词向量求和取平均即可，即： x_w = \\frac{1}{2c}\\sum\\limits_{i=1}^{2c}x_i第二步，通过梯度上升法来更新我们的$\\theta_{j-1}^w$和$x_w$，注意这里的$x_w$是由$2c$个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个$x_i(i=1,2,,,,2c)$，即： \\theta_{j-1}^w = \\theta_{j-1}^w + \\eta (1-d_j^w-\\sigma(x_w^T\\theta_{j-1}^w))x_w x_i= x_i +\\eta \\sum\\limits_{j=2}^{l_w}(1-d_j^w-\\sigma(x_w^T\\theta_{j-1}^w))\\theta_{j-1}^w \\;(i =1,2..,2c)其中$η$为梯度上升法的步长。 这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法： 输入：基于CBOW的语料训练样本，词向量的维度大小$M$，CBOW的上下文大小$2c$,步长$η$ 输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$w$ 基于语料训练样本建立霍夫曼树。 随机初始化所有的模型参数$θ$，所有的词向量$w$ 进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理： 3.1. e=0，计算$x_w= \\frac{1}{2c}\\sum\\limits_{i=1}^{2c}x_i$ 3.2. for j = 2 to $l_w$ 计算： f = \\sigma(x_w^T\\theta_{j-1}^w)\\\\ g = (1-d_j^w-f)\\eta\\\\ e = e + g\\theta_{j-1}^w\\\\ \\theta_{j-1}^w= \\theta_{j-1}^w + gx_w3.3. 对于$context(w)$中的每一个词向量$x_i$(共$2c$个)进行更新： xi=xi+e3.4. 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。 6、基于Hierarchical Softmax的Skip-Gram模型现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词$w$,输出的为$2c$c个词向量$context(w)$。 我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的$c$个词和后面的$c$个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。 Skip-Gram模型和CBOW模型其实是反过来的，在上一篇已经讲过。 在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。 对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即$x_w$就是词$w$对应的词向量。 第二步，通过梯度上升法来更新我们的$θ^w_{j−1}$和$x_w$，注意这里的$x_w$周围有$2c$个词向量，此时如果我们期望$P(x_i|x_w),i=1,2…2c$最大。此时我们注意到由于上下文是相互的，在期望$P(x_i|x_w),i=1,2…2c$最大化的同时，反过来我们也期望$P(x_w|x_i),i=1,2…2c$最大。那么是使用$P(x_i|x_w)$好还是$P(x_w|x_i)$好呢，word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新$x_w$一个词，而是$x_i,i=1,2…2c$共$2c$个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对$2c$个输出进行迭代更新。 这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法： 输入：基于Skip-Gram的语料训练样本，词向量的维度大小$M$，Skip-Gram的上下文大小$2c$,步长$η$ 输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$w$ 基于语料训练样本建立霍夫曼树。 随机初始化所有的模型参数$θ$，所有的词向量$w$, 进行梯度上升迭代过程，对于训练集中的每一个样本$(w,context(w))$做如下处理： 3.1. for i =1 to $2c$: i) e=0 ​ ii) for j = 2 to $l_w$, 计算： f = \\sigma(x_i^T\\theta_{j-1}^w)\\\\ g = (1-d_j^w-f)\\eta\\\\ e = e + g\\theta_{j-1}^w\\\\ \\theta_{j-1}^w= \\theta_{j-1}^w+ gx_i​ iii) x_i = x_i + e3.2. 如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤3.1继续迭代。 7、Hierarchical Softmax的模型源码和算法的对应这里给出上面算法和word2vec源码中的变量对应关系。 在源代码中，基于Hierarchical Softmax的CBOW模型算法在435-463行，基于Hierarchical Softmax的Skip-Gram的模型算法在495-519行。大家可以对着源代码再深入研究下算法。 在源代码中，$neule$对应我们上面的$e$, $syn0$对应我们的$x_w$, $syn1$对应我们的$θ^i_{j−1}$, $layer1_size$对应词向量的维度，$window$对应我们的$c$。 另外，$vocab[word].code[d]$指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。 以上就是基于Hierarchical Softmax的word2vec模型，下一篇我们讨论基于Negative Sampling的word2vec模型。 8、Hierarchical Softmax的缺点与改进在讲基于Negative Sampling的word2vec模型前，我们先看看Hierarchical Softmax的的缺点。的确，使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？ Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解，下面我们就来看看Negative Sampling的求解思路. 9、基于Negative Sampling的模型概述既然名字叫Negative Sampling（负采样），那么肯定使用了采样的方法。采样的方法有很多种，比如之前讲到的大名鼎鼎的MCMC。我们这里的Negative Sampling采样方法并没有MCMC那么复杂。 比如我们有一个训练样本，中心词是$w$,它周围上下文共有$2c$个词，记为$context(w)$。由于这个中心词$w$,的确和$context(w)$相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到$neg$个和$w$不同的中心词$w_i,i=1,2,..neg$，这样$context(w)$和$w_i$就组成了$neg$个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词$w_i$对应的模型参数$θ_i$，和每个词的词向量。 从上面的描述可以看出，Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。 不过有两个问题还需要弄明白：1）如果通过一个正例和neg个负例进行二元逻辑回归呢？ 2）如何进行负采样呢？ 10、基于Negative Sampling的模型梯度计算Negative Sampling也是采用了二元逻辑回归来求解模型参数，通过负采样，我们得到了neg个负例$(context(w),w_i), i=1,2,..neg$。为了统一描述，我们将正例定义为$w_0$。 在逻辑回归中，我们的正例应该期望满足： P(context(w_0), w_i) = \\sigma(x_{w_0}^T\\theta^{w_i}) ,y_i=1, i=0我们的负例期望满足： P(context(w_0), w_i) =1- \\sigma(x_{w_0}^T\\theta^{w_i}), y_i = 0, i=1,2,..neg我们期望可以最大化下式： \\prod_{i=0}^{neg}P(context(w_0), w_i) = \\sigma(x_{w_0}^T\\theta^{w_0})\\prod_{i=1}^{neg}(1- \\sigma(x_{w_0}^T\\theta^{w_i}))利用逻辑回归和上一节的知识，我们容易写出此时模型的似然函数为： \\prod_{i=0}^{neg} \\sigma(x_{w_0}^T\\theta^{w_i})^{y_i}(1- \\sigma(x_{w_0}^T\\theta^{w_i}))^{1-y_i}和Hierarchical Softmax类似，我们采用随机梯度上升法，仅仅每次只用一个样本更新梯度，来进行迭代更新得到我们需要的$x_{wi},θ_{wi},i=0,1,..neg$, 这里我们需要求出$x_{w0},θ_{wi},i=0,1,..neg$的梯度。 首先我们计算$θ_{wi}$的梯度： \\begin{align} \\frac{\\partial L}{\\partial \\theta^{w_i} } &= y_i(1- \\sigma(x_{w_0}^T\\theta^{w_i}))x_{w_0}-(1-y_i)\\sigma(x_{w_0}^T\\theta^{w_i})x_{w_0} \\\\ & = (y_i -\\sigma(x_{w_0}^T\\theta^{w_i})) x_{w_0} \\end{align}同样的方法，我们可以求出$x_{w0}$的梯度如下： \\frac{\\partial L}{\\partial x^{w_0} } = \\sum\\limits_{i=0}^{neg}(y_i -\\sigma(x_{w_0}^T\\theta^{w_i}))\\theta^{w_i}有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的$x_{w_0},θ_{w_i},i=0,1,..neg$。 11、Negative Sampling负采样方法现在我们来看看如何进行负采样，得到neg个负例。word2vec采样的方法并不复杂，如果词汇表的大小为$V$,那么我们就将一段长度为1的线段分成$V$份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。每个词$w$的线段长度由下式决定： len(w) = \\frac{count(w)}{\\sum\\limits_{u \\in vocab} count(u)}在word2vec中，分子和分母都取了3/4次幂如下： len(w) = \\frac{count(w)^{3/4}}{\\sum\\limits_{u \\in vocab} count(u)^{3/4}}在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M&gt;&gt;V$，这样可以保证每个词对应的线段都会划分成对应的小块。而$M$份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从$M$个位置中采样出$neg$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。 在word2vec中，MM取值默认为$10^8$。 12、基于Negative Sampling的CBOW模型有了上面Negative Sampling负采样的方法和逻辑回归求解模型参数的方法，我们就可以总结出基于Negative Sampling的CBOW模型算法流程了。梯度迭代过程使用了随机梯度上升法： 输入：基于CBOW的语料训练样本，词向量的维度大小$Mcount$，CBOW的上下文大小$2c$,步长$η$, 负采样的个数neg 输出：词汇表每个词对应的模型参数$θ$，所有的词向量$x_w$ 随机初始化所有的模型参数$θ$，所有的词向量$w$ 对于每个训练样本$(context(w_0),w_0)$,负采样出neg个负例中心词$w_i,i=1,2,…neg$ 进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w_0),w_0,w_1,…w_neg)$做如下处理： a). e=0， 计算$x_{w_0}= \\frac{1}{2c}\\sum\\limits_{i=1}^{2c}x_i$ b). for i= 0 to neg, 计算： f = \\sigma(x_{w_0}^T\\theta^{w_i})\\\\ g = (y_i-f)\\eta\\\\ e = e + g\\theta^{w_i}\\\\ \\theta^{w_i}= \\theta^{w_i} + gx_{w_0}c). 对于$context(w)$中的每一个词向量$x_k$(共2c个)进行更新： x_k = x_k + ed). 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。 13、基于Negative Sampling的Skip-Gram模型有了上一节CBOW的基础和上一篇基于Hierarchical Softmax的Skip-Gram模型基础，我们也可以总结出基于Negative Sampling的Skip-Gram模型算法流程了。梯度迭代过程使用了随机梯度上升法： 输入：基于Skip-Gram的语料训练样本，词向量的维度大小$Mcount$，Skip-Gram的上下文大小$2c$,步长$η$，负采样的个数neg。 输出：词汇表每个词对应的模型参数$θ$，所有的词向量$x_w$ 随机初始化所有的模型参数$θ$，所有的词向量$w$ 对于每个训练样本$(context(w0),w0)$,负采样出neg个负例中心词$w_i,i=1,2,…neg$ 进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w_0),w_0,w_1,…w_neg)$做如下处理： 3.1. for i =1 to 2c: ​ i) e=0 ​ ii) for j= 0 to neg, 计算： f = \\sigma(x_{w_{0i}}^T\\theta^{w_j})\\\\ g = (y_j-f)\\eta\\\\ e = e + g\\theta^{w_j}\\\\ \\theta^{w_j}= \\theta^{w_j} + gx_{w_{0i}}​ iii) 词向量更新： x_{w_{0i}} = x_{w_{0i}} + e3.2. 如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤3.1继续迭代。 14、Negative Sampling的模型源码和算法的对应这里给出上面算法和word2vec源码中的变量对应关系。 在源代码中，基于Negative Sampling的CBOW模型算法在464-494行，基于Negative Sampling的Skip-Gram的模型算法在520-542行。大家可以对着源代码再深入研究下算法。 在源代码中，neule对应我们上面的$e$, syn0对应我们的$x_w$, $syn1neg$对应我们的$θ_{wi}$, layer1_size对应词向量的维度，window对应我们的$c$。negative对应我们的neg, table_size对应我们负采样中的划分数$M$。 另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。这些和基于Hierarchical Softmax的是一样的。 以上就是基于Negative Sampling的word2vec模型，希望可以帮到大家，后面会讲解用gensim的python版word2vec来使用word2vec解决实际问题。"},{"title":"霍夫曼树","date":"2021-02-25T02:16:55.000Z","path":"2021/02/25/霍夫曼树/","text":"转载自https://www.jianshu.com/p/5ad3e97d54a3 前言霍夫曼树是二叉树的一种特殊形式，又称为最优二叉树，其主要作用在于数据压缩和编码长度的优化。 重要概念路径和路径长度在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支的数目称为路径长度。若规定根结点的层数为1，则从根结点到第L层结点的路径长度为L-1。 上图所示二叉树结点A到结点D的路径长度为2，结点A到达结点C的路径长度为1。 结点的权及带权路径长度若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。结点的带权路径长度为：从根结点到该结点之间的路径长度与该结点的权的乘积。下图稀罕事了一棵带权的二叉树。 树的带权路径长度树的带权路径长度规定为所有叶子结点的带权路径长度之和，记为WPL。上图所示二叉树的WPL：WPL = 6 * 2 + 3 * 2 + 8 * 2 = 34； 霍夫曼树定义给定n个权值作为n个叶子结点，构造一棵二叉树，若带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为霍夫曼树(Huffman Tree)。如下图所示两棵二叉树。 叶子结点为A、B、C、D，对应权值分别为7、5、2、4。3.1.a树的WPL = 7 * 2 + 5 * 2 + 2 * 2 + 4 * 2 = 36，3.1.b树的WPL = 7 * 1 + 5 * 2 + 2 * 3 + 4 * 3 = 35。由ABCD构成叶子结点的二叉树形态有许多种，但是WPL最小的树只有3.1.b所示的形态。则3.1.b树为一棵霍夫曼树。 构造霍夫曼树构造霍夫曼树主要运用于编码，称为霍夫曼编码。现考虑使用3.1中ABCD结点以及对应的权值构成如下长度编码。AACBCAADDBBADDAABB。编码规则为：从根节点出发，向左标记为0，向右标记为1。采用上述编码规则，将上图编码为下图所示： 构造过程：3.1.a所示二叉树称为等长编码，由于共有4个结点，故需要2位编码来表示，编码结果为： 结点 编码 A 00 B 01 C 10 D 11 则AACBCAADDBBADDAABB对应编码为：00 00 10 01 10 00 00 11 11 01 01 00 11 11 00 00 01 01，长度为36。 3.1.b构造过程如下：1）选择结点权值最小的两个结点构成一棵二叉树如图3.3： 2）则现在可以看作由T1，A，B构造霍夫曼树，继续执行步骤1。选则B和T1构成一棵二叉树如图3.4： 3）现只有T2和A两个结点，继续执行步骤1。选择A和T2构成一棵二叉树如图3.5： 经过上述步骤则可以构造完一棵霍夫曼树。通过观察可以发现，霍夫曼树中权值越大的结点距离根结点越近。按照图3.5霍夫曼树编码结果： 结点 编码 A 0 B 10 C 110 D 111 则AACBCAADDBBADDAABB对应编码为：0 0 110 10 110 0 0 111 111 10 10 0 111 111 0 0 10 10，编码长度为35。由此可见，采用二叉树可以适当降低编码长度，尤其是在编码长度较长，且权值分布不均匀时，采用霍夫曼编码可以大大缩短编码长度。 代码实现后续补充"},{"title":"使用meilisearch搭建rust搜索引擎","date":"2021-02-24T12:27:37.000Z","path":"2021/02/24/使用meilisearch搭建rust搜索引擎/","text":"简介meilissearch和Elasticsearch的主要区别是什么?为什么它们很重要?自2010年创建以来，Elasticsearch迅速成为搜索引擎市场的热门选项，在各种应用程序和数据集中都有使用。然而，不可否认的是，对于繁重的全文搜索来说，它是一个强大的工具，但在使用之前需要进行广泛的配置，这意味着所有这些功能都伴随着高昂的资源成本。对于处理大量日志的大型科技公司来说，Elasticsearch可能是有意义的。但对于中等规模的数据集(即少于500万行)和较小的应用程序，它就不适合这份工作。尽管如此，由于历史上没有开源的替代方案，开发人员仍然选择弹性作为他们的默认解决方案，最终在设置和培训上损失了不成比例的大量时间。这就是“Meilisearch”的用武之地。MeiliSearch提供了一个快速、相关和容错的搜索，并且几乎不需要设置时间，对于任何需要一个强大且可访问的搜索引擎的开发人员来说，它都是一个很好的解决方案。"},{"title":"zulip聊天记录爬取","date":"2021-02-24T10:43:23.000Z","path":"2021/02/24/zulip聊天记录爬取/","text":"出于团队rust洞察任务需求，需要研发出zulip聊天记录抓取和提取工具。我的系统是ubuntu20，按照网上的教程搜索安装zulip，注意配置rc文件即可。 接下来需要导出zulip的聊天记录。以rust治理组的zulip聊天室为例，首先设定一个channel列表，代表不同的聊天频道。 接下来调用python的zulip包，首先调用zulip.client(config_file)，之后调用client.get_messages(request)来获得返回值。这里request中可以修改获取聊天记录的数量和起止时间。将这些聊天记录根据聊天室名称分别保存。 将聊天记录保存为文件之后，根据词匹配、切分等方法，将信息提取为“发送者姓名”，“发送者邮箱”，“时间”，“主题”，“内容”五个维度，并将其存入json文件中。 整体下来，获取过程还是比较简单的。由于zulip python库的存在，我们不需要另外去请求连接了。信息获取下来之后，可以使用关键词提取等方法，获取聊天记录热词等。"},{"title":"github项目信息提取","date":"2021-02-24T08:18:13.000Z","path":"2021/02/24/github项目信息提取/","text":"之前对Github用户主页进行了信息提取，这次对Github项目信息进行提取。由于项目的特殊性，不仅存在基本信息，也存在贡献者、commit等信息。这些信息如果通过url访问将是非常巨大的工作量，因此可以采用将仓库克隆到本地使用git解析的方法来进行这类信息的提取。综上，对于一个项目的信息提取包括以下步骤。 访问仓库网页地址，例如https://github.com/rust-lang/rust，并使用BeautifulSoup对返回的网页进行解析。 将仓库使用git clone命令保存到本地，然后使用git --no-pager log --stat &gt;&gt; commit_file.txt命令保存commit记录，然后从这个文件中提取贡献者和具体的贡献记录。 解析网页的内容相对比较简单，使用BeautifulSoup提取标签内容、标签属性等即可，跟之前提取用户主页信息类似。 解析commit信息相对复杂一些，需要使用关键词筛选和正则匹配等方法，将包含commit信息的行提取出来，并根据不同的贡献者、不同的文件路径（注意不仅仅是文件名，要更高级一点）分别统计总体的代码贡献行数。这里有个有趣的发现，commit信息中包含了非常重要的开发者联系方式，即邮箱，这个可是在外面难以搜索到的。emmmmmm。。。 最终将所有的信息存储到project字典中，一级字段包括summary、contributors、file_commits。该dict可以另存为json文件，方便进行其他操作。具体的代码见https://github.com/CAKGOD/myCode/blob/master/examples/github/github_project_homepage_extraction.ipynb。 本次Github项目信息提取的效果还是不错的，不过还是有一些可以改进的地方。例如可以添加根据时间进行统计的功能，毕竟有时候我们只是想统计最近一段时间的贡献历史。这个项目信息统计出来之后，可以用来找出关键贡献者（例如使用半衰期排名算法，明后天更新一下）、关键代码文件等。"},{"title":"github用户主页信息提取","date":"2021-02-23T14:10:17.000Z","path":"2021/02/23/github用户主页信息提取/","text":"Github真的是一个非常好的网站。从Github的用户个人主页可以看到用户公开的信息，也可以体现出用户的技术栈、兴趣爱好、最近的动态等等。为了完整地对一个用户进行画像，需要有完整的数据来进行支撑，这就需要对Github的个人主页进行信息提取。本次我仅仅解析了个人的主页部分，就是一打开某个人的Github的那个页面。 Github提供了相应的api可以访问用户的一些信息，但是本次使用了BeautifulSoup来进行解析。首先使用requests模块访问Github主页，这里有一个注意点，如果只是想获得用户的基本信息，直接访问url：https://github.com/CAKGOD；如果需要获取用户的Activity例如pr、commit等信息，需要访问url：https://github.com/CAKGOD?tab=overview&amp;from=2019-12-01&amp;to=2019-12-31，其中的日期根据自己的需要进行修改。例如我想根据某个用户的月度贡献来进行统计，就把上面from和to的值间隔改成一个月。 获得request的返回网页后，使用BeautifulSoup进行解析，其中涉及到一些标签选择、属性值抽取的操作，这些操作按照自己的需求和BeautifulSoup使用文档操作即可。最终我将整个页面归纳为一个dict，其中第一层key为basic和activity，其中basic的二层key有ava、name、id、slogan、followers、following、star、worksFor、homeLocation、email、url、twitter、highlight、Organizations、Sponsoring、pin_popular，Activity的二层key根据月份来对五种活动进行统计。 具体的代码见https://github.com/CAKGOD/myCode/blob/master/examples/github/github_homepage_extract.ipynb 这次的解析效果还是不错的，不过还是有一些不足之处。例如基本信息的统计维度不完整，用户Activity的类型种类不完整，pinned的项目的语言、star数量、fork数量没有提取。不过这些可以后续完善。"},{"title":"javascript使用总结","date":"2021-02-08T07:08:20.000Z","path":"2021/02/08/javascript使用总结/","text":"JavaScript使用总结‘use strict’“use strict” 指令在 JavaScript 1.8.5 (ECMAScript5) 中新增。它不是一条语句，但是是一个字面量表达式，在 JavaScript 旧版本中会被忽略。”use strict” 的目的是指定代码在严格条件下执行。严格模式下你不能使用未声明的变量。 使用严格模式的优势可以消除Javascript语法的一些不合理、不严谨之处，减少一些怪异行为； 消除代码运行的一些不安全之处，保证代码运行的安全； 提高编译器效率，增加运行速度； 为未来新版本的Javascript做好铺垫。 “严格模式”体现了Javascript更合理、更安全、更严谨的发展方向，包括IE 10在内的主流浏览器，都已经支持它，许多大项目已经开始全面拥抱它。 setTimeoutsetTimeout() 是属于 window 的方法，该方法用于在指定的毫秒数后调用函数或计算表达式。语法格式可以是以下两种： setTimeout(要执行的代码, 等待的毫秒数) setTimeout(JavaScript 函数, 等待的毫秒数) setTimeout() 是设定一个指定等候时间（单位是千分之一秒，millisecond）, 时间到了，浏览器就会执行一个指定的代码。 varvar 语句用于声明变量。JavaScript变量的创建也叫作“声明“一变量： var carName; 变量声明后，变量为空 (没有值)。为变量赋值，操作如下: carName = &quot;Volvo&quot;; 声明变量时，你同样可以为变量赋值： var carName = &quot;Volvo&quot;; Windowwindow对象所有浏览器都支持 window 对象，它表示浏览器窗口。所有 JavaScript 全局对象、函数以及变量均自动成为 window 对象的成员。 全局变量是 window 对象的属性。全局函数是 window 对象的方法。甚至 HTML DOM 的 document 也是 window 对象的属性之一： window.document.getElementById(&quot;header&quot;); 与此相同： document.getElementById(&quot;header&quot;); window尺寸有三种方法能够确定浏览器窗口的尺寸。对于Internet Explorer、Chrome、Firefox、Opera 以及 Safari： window.innerHeight - 浏览器窗口的内部高度(包括滚动条) window.innerWidth - 浏览器窗口的内部宽度(包括滚动条) 对于 Internet Explorer 8、7、6、5： document.documentElement.clientHeight document.documentElement.clientWidth或者 document.body.clientHeight document.body.clientWidth 其他window方法 window.open() - 打开新窗口 window.close() - 关闭当前窗口 window.moveTo() - 移动当前窗口 window.resizeTo() - 调整当前窗口的尺寸 其他"},{"title":"github-api使用总结","date":"2021-02-05T11:42:33.000Z","path":"2021/02/05/github-api使用总结/","text":"github-api使用总结 github search api 找到过去一段时间内最热的几个项目 $ curl -G https://api.github.com/search/repositories \\ --data-urlencode &quot;q=created:2021-01-01&quot; \\ --data-urlencode &quot;sort=stars&quot; \\ --data-urlencode &quot;order=desc&quot; \\ -H &quot;Accept: application/vnd.github.preview&quot; \\ | jq &quot;.items[0,1,2] | &#123;name, description, language, watchers_count, html_url&#125;&quot; 找到最早的用户账号并且没有人follow的github账号（人类清除计划hhh） $ curl -G https://api.github.com/search/users \\ --data-urlencode &#39;q=followers:0&#39; \\ --data-urlencode &#39;sort=joined&#39; \\ --data-urlencode &#39;order=asc&#39; \\ -H &#39;Accept: application/vnd.github.preview&#39; \\ | jq &#39;.items[0,1,2] | &#123;html_url, login, id&#125;&#39;"},{"title":"markdown使用总结","date":"2021-02-04T02:48:26.000Z","path":"2021/02/04/markdown使用总结/","text":"Markdown使用教程1、简介2、Markdown标题Markdown 标题有两种格式。 （1）使用 = 和 - 标记一级和二级标题= 和 - 标记语法格式如下： 我展示的是一级标题 ================= 我展示的是二级标题 ----------------- （2）使用 # 号标记# 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 3、Markdown 段落Markdown 段落没有特殊的格式，直接编写文字就好，段落的换行是使用两个以上空格加上回车。 当然也可以在段落后面使用一个空行来表示重新开始一个段落。 4、Markdown字体Markdown 可以使用以下几种字体： *斜体文本* _斜体文本_ **粗体文本** __粗体文本__ ***粗斜体文本*** ___粗斜体文本___ 5、Markdown分割线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线： *** * * * ***** - - - ---------- 6、Markdown删除线如果段落上的文字要添加删除线，只需要在文字的两端加上两个波浪线 ~~ 即可，实例如下： RUNOOB.COM GOOGLE.COM ~~BAIDU.COM~~ 7、Markdown下划线下划线可以通过 HTML 的 标签来实现： &lt;u&gt;带下划线文本&lt;/u&gt; 8、Markdown脚注脚注是对文本的补充说明。 Markdown 脚注的格式如下: [^要注明的文本] 以下实例演示了脚注的用法： 创建脚注格式类似这样 [^RUNOOB]。 [^RUNOOB]: 菜鸟教程 -- 学的不仅是技术，更是梦想！！！ 9、Markdown列表Markdown 支持有序列表和无序列表。 无序列表使用星号(*)、加号(+)或是减号(-)作为列表标记，这些标记后面要添加一个空格，然后再填写内容： * 第一项 * 第二项 * 第三项 + 第一项 + 第二项 + 第三项 - 第一项 - 第二项 - 第三项 有序列表使用数字并加上 . 号来表示，如： 1. 第一项 2. 第二项 3. 第三项 列表嵌套只需在子列表中的选项前面添加四个空格即可： 1. 第一项： - 第一项嵌套的第一个元素 - 第一项嵌套的第二个元素 2. 第二项： - 第二项嵌套的第一个元素 - 第二项嵌套的第二个元素 10、Markdown区块Markdown 区块引用是在段落开头使用 &gt; 符号 ，然后后面紧跟一个空格符号： &gt; 区块引用 &gt; 菜鸟教程 &gt; 学的不仅是技术更是梦想 另外区块是可以嵌套的，一个 &gt; 符号是最外层，两个 &gt; 符号是第一层嵌套，以此类推： &gt; 最外层 &gt; &gt; 第一层嵌套 &gt; &gt; &gt; 第二层嵌套 区块中使用列表实例如下： &gt; 区块中使用列表 &gt; 1. 第一项 &gt; 2. 第二项 &gt; + 第一项 &gt; + 第二项 &gt; + 第三项 如果要在列表项目内放进区块，那么就需要在 &gt; 前添加四个空格的缩进。 区块中使用列表实例如下： * 第一项 &gt; 菜鸟教程 &gt; 学的不仅是技术更是梦想 * 第二项 11、Markdown代码如果是段落上的一个函数或片段的代码可以用反引号把它包起来（`），例如： `printf()` 函数 代码区块使用 4 个空格或者一个制表符（Tab 键）。 也可以用 ``` 包裹一段代码，并指定一种语言（也可以不指定）： ​```javascript $(document).ready(function () &#123; alert(&#39;RUNOOB&#39;); &#125;); ​ ## 12、Markdown链接 链接使用方法如下： 链接名称 或者 &lt;链接地址&gt; 例如： 这是一个链接 菜鸟教程 可以通过变量来设置一个链接，变量赋值在文档末尾进行： 这个链接用 1 作为网址变量 Google这个链接用 runoob 作为网址变量 Runoob然后在文档的结尾为变量赋值（网址） ## 13、Markdown图片 Markdown 图片语法格式如下： - 开头一个感叹号 ! - 接着一个方括号，里面放上图片的替代文字 - 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上选择性的 &#39;title&#39; 属性的文字。 使用示例： 当然，也可以像网址那样对图片网址使用变量:. 这个链接用 1 作为网址变量 RUNOOB.然后在文档的结尾为变量赋值（网址） Markdown 还没有办法指定图片的高度与宽度，如果你需要的话，你可以使用普通的 &lt;img&gt; 标签。 ## 14、Markdown表格 Markdown 制作表格使用 **|** 来分隔不同的单元格，使用 **-** 来分隔表头和其他行。 语法格式如下： 表头 表头 单元格 单元格 单元格 单元格 **可以设置表格的对齐方式：** - **-:** 设置内容和标题栏居右对齐。 - **:-** 设置内容和标题栏居左对齐。 - **:-:** 设置内容和标题栏居中对齐。 示例如下： 左对齐 右对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 ## 15、Markdown 高级技巧 不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML 撰写。 目前支持的 HTML 元素有：`&lt;kbd&gt; &lt;b&gt; &lt;i&gt; &lt;em&gt; &lt;sup&gt; &lt;sub&gt; &lt;br&gt;`等 ，如： 使用 Ctrl+Alt+Del 重启电脑 Markdown 使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown 使用反斜杠转义特殊字符： 文本加粗** 正常显示星号 ** Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： \\ 反斜线` 反引号 星号_ 下划线{} 花括号[] 方括号() 小括号井字号 加号 减号. 英文句点! 感叹号``` 当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。如： $$ \\mathbf&#123;V&#125;_1 \\times \\mathbf&#123;V&#125;_2 = \\begin&#123;vmatrix&#125; \\mathbf&#123;i&#125; &amp; \\mathbf&#123;j&#125; &amp; \\mathbf&#123;k&#125; \\\\ \\frac&#123;\\partial X&#125;&#123;\\partial u&#125; &amp; \\frac&#123;\\partial Y&#125;&#123;\\partial u&#125; &amp; 0 \\\\ \\frac&#123;\\partial X&#125;&#123;\\partial v&#125; &amp; \\frac&#123;\\partial Y&#125;&#123;\\partial v&#125; &amp; 0 \\\\ \\end&#123;vmatrix&#125; $&#123;$tep1&#125;&#123;\\style&#123;visibility:hidden&#125;&#123;(x+1)(x+1)&#125;&#125; $$"},{"title":"使用turtlebot2和速腾聚创雷达搭建移动机器人数据采集平台","date":"2021-02-04T01:47:46.000Z","path":"2021/02/04/使用turtlebot2和速腾聚创雷达搭建移动机器人数据采集平台/","text":"之前一段时间在Navigation2社区进行贡献，鉴于目前通用的数据集是用于自动驾驶的，社区Leader Steve对移动机器人数据集提出了需求，同时提出了需要考虑到多数开发者控制成本的要求。为此，特意购置了速腾聚创的16线雷达，以及Turtlebot2移动机器人来组装该平台。 硬件部分整套设备由速腾聚创雷达、Xsens惯导、Turtlebot2、Mini PC以及两块电源组成。为了保证雷达有足够的视野，需要将雷达安装在Turtlebot2的最顶部，将电源通过转接器进行电压转化对雷达进行供电。Xsens固定在雷达垂直的位置上，保证相对位置固定，通过Mini PC的数据线连接供电。Mini PC通过同样电源的方法进行供电，并通过网线和雷达连接，通过数据线和Turtlebot连接。Turtlebot本身充满电即可。最后的组装的完整设备如下图所示。 软件部分 在Mini PC上安装相关软件 安装ubuntu 18.04 安装ROS melodic 安装Turtlebot相关软件sudo apt install ros-melodic-kobuki-*sudo apt install ros-melodic-ecl-streamssudo apt install libusb-devsudo apt install libspnav-devsudo apt install ros-melodic-joystick-driverssudo apt install libbluetooth-devsudo apt install libcwiid-devsudo apt install ros-melodic-robot-pose-ekf 准备SDK软件mkdir -p ~/turtlebot_ws/srccd ~/turtlebot_ws/srcgit clone https://github.com/turtlebot/turtlebot_simulatorgit clone https://github.com/turtlebot/turtlebot.git​git clone https://github.com/turtlebot/turtlebot_apps.gitgit clone https://github.com/udacity/robot_pose_ekf​git clone https://github.com/ros-perception/depthimage_to_laserscan.gitgit clone https://github.com/yujinrobot/kobuki_msgs.gitgit clone https://github.com/yujinrobot/kobuki_desktop.gitcd kobuki_desktop/​rm -r kobuki_qtestsuitegit clone https://github.com/toeklk/orocos-bayesian-filtering.gitgit clone https://github.com/turtlebot/turtlebot_msgs.gitgit clone https://github.com/ros-drivers/joystick_drivers.git 将kobuki和yujin_ocs依赖复制到turtlebot/src/​mkdir -p ~/repos/cd ~/repos/​git clone https://github.com/yujinrobot/kobuki.git​cp -r kobuki/* ~/turtlebot_ws/src/​git clone https://github.com/yujinrobot/yujin_ocs.git​cp -r yujin_ocs/yocs_cmd_vel_mux/ yujin_ocs/yocs_controllers ~/turtlebot_ws/src/ 编译 cd ~/turtlebot_wssource /opt/ros/melodic/setup.bashcatkin_make 将工作空间添加到bashrc文件中echo &quot;source ~/turtlebot_ws/devel/setup.bash&quot; &gt;&gt; ~/.bashrc source `source ~/.bashrc 安装Xsens惯导软件 首先安装mti670驱动，从MT SDK文件夹中将 xsens_ros_mti_driver文件夹复制到catkin工作目录的src文件夹中，使用chmod 777命令对所有文件夹和文件添加权限，然后使用pushd src/xsens_ros_mti_driver/lib/xspublic &amp;&amp; make &amp;&amp; popd命令编译xspublic，最后使用catkin_make编译Mti驱动文件夹。 其他 使用 将各个线连接好 启动Turtlebot2 roslaunch turtlebot_bringup minimal.launch 将会看以下信息 ubuntu@AiROS-XA:~/turtlebot_ws/src$ roslaunch turtlebot_bringup minimal.launch ... logging to /home/ubuntu/.ros/log/a22c211a-7888-11e9-8396-00044bcb943f/roslaunch-AiROS- XA-6402.log Checking log directory for disk usage. This may take awhile. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is &lt;1GB. xacro: in-order processing became default in ROS Melodic. You can drop the option. started roslaunch server http://AiROS-XA:41151/ SUMMARY ======== PARAMETERS * /bumper2pointcloud/pointcloud_radius: 0.24 * /cmd_vel_mux/yaml_cfg_file: /home/ubuntu/turt... * /diagnostic_aggregator/analyzers/input_ports/contains: [&#39;Digital Input&#39;,... * /diagnostic_aggregator/analyzers/input_ports/path: Input Ports * /diagnostic_aggregator/analyzers/input_ports/remove_prefix: mobile_base_nodel... * /diagnostic_aggregator/analyzers/input_ports/timeout: 5.0 * /diagnostic_aggregator/analyzers/input_ports/type: diagnostic_aggreg... * /diagnostic_aggregator/analyzers/kobuki/contains: [&#39;Watchdog&#39;, &#39;Mot... * /diagnostic_aggregator/analyzers/kobuki/path: Kobuki * /diagnostic_aggregator/analyzers/kobuki/remove_prefix: mobile_base_nodel... * /diagnostic_aggregator/analyzers/kobuki/timeout: 5.0 * /diagnostic_aggregator/analyzers/kobuki/type: diagnostic_aggreg... * /diagnostic_aggregator/analyzers/power/contains: [&#39;Battery&#39;, &#39;Lapt... * /diagnostic_aggregator/analyzers/power/path: Power System * /diagnostic_aggregator/analyzers/power/remove_prefix: mobile_base_nodel... * /diagnostic_aggregator/analyzers/power/timeout: 5.0 * /diagnostic_aggregator/analyzers/power/type: diagnostic_aggreg... * /diagnostic_aggregator/analyzers/sensors/contains: [&#39;Cliff Sensor&#39;, ... * /diagnostic_aggregator/analyzers/sensors/path: Sensors * /diagnostic_aggregator/analyzers/sensors/remove_prefix: mobile_base_nodel... * /diagnostic_aggregator/analyzers/sensors/timeout: 5.0 * /diagnostic_aggregator/analyzers/sensors/type: diagnostic_aggreg... * /diagnostic_aggregator/base_path: * /diagnostic_aggregator/pub_rate: 1.0 * /mobile_base/base_frame: base_footprint * /mobile_base/battery_capacity: 16.5 * /mobile_base/battery_dangerous: 13.2 * /mobile_base/battery_low: 14.0 * /mobile_base/cmd_vel_timeout: 0.6 * /mobile_base/device_port: /dev/kobuki * /mobile_base/odom_frame: odom * /mobile_base/publish_tf: True * /mobile_base/use_imu_heading: True * /mobile_base/wheel_left_joint_name: wheel_left_joint * /mobile_base/wheel_right_joint_name: wheel_right_joint * /robot/name: turtlebot * /robot/type: turtlebot * /robot_description: &lt;?xml version=&quot;1.... * /robot_state_publisher/publish_frequency: 5.0 * /rosdistro: melodic * /rosversion: 1.14.3 * /use_sim_time: False NODES / bumper2pointcloud (nodelet/nodelet) cmd_vel_mux (nodelet/nodelet) diagnostic_aggregator (diagnostic_aggregator/aggregator_node) mobile_base (nodelet/nodelet) mobile_base_nodelet_manager (nodelet/nodelet) robot_state_publisher (robot_state_publisher/robot_state_publisher) auto-starting new master process[master]: started with pid [6415] ROS_MASTER_URI=http://localhost:11311 setting /run_id to a22c211a-7888-11e9-8396-00044bcb943f process[rosout-1]: started with pid [6426] started core service [/rosout] process[robot_state_publisher-2]: started with pid [6434] process[diagnostic_aggregator-3]: started with pid [6435] process[mobile_base_nodelet_manager-4]: started with pid [6440] process[mobile_base-5]: started with pid [6444] process[bumper2pointcloud-6]: started with pid [6448] process[cmd_vel_mux-7]: started with pid [6451] 启动键盘控制 运行$roslaunch turtlebot_teleop keyboard_teleop.launch，屏幕上会出现以下信息： ubuntu@AiROS-XA:~$ roslaunch turtlebot_teleop keyboard_teleop.launch ... logging to /home/ubuntu/.ros/log/a22c211a-7888-11e9-8396-00044bcb943f/roslaunch-AiROS-XA-6533.log Checking log directory for disk usage. This may take awhile. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is &lt;1GB. started roslaunch server http://AiROS-XA:41229/ SUMMARY ======== PARAMETERS * /rosdistro: melodic * /rosversion: 1.14.3 * /turtlebot_teleop_keyboard/scale_angular: 1.5 * /turtlebot_teleop_keyboard/scale_linear: 0.5 NODES / turtlebot_teleop_keyboard (turtlebot_teleop/turtlebot_teleop_key) ROS_MASTER_URI=http://localhost:11311 process[turtlebot_teleop_keyboard-1]: started with pid [6552] Control Your Turtlebot! --------------------------- Moving around: u i o j k l m , . q/z : increase/decrease max speeds by 10% w/x : increase/decrease only linear speed by 10% e/c : increase/decrease only angular speed by 10% space key, k : force stop anything else : stop smoothly CTRL-C to quit currently: speed 0.2 turn 1 启动雷达 通过$roslaunch rslidar_pointcloud rs_lidar_32.launch启动雷达和Rviz并且此时可以再Rviz中看到点云。 启动Xsens惯导 roslaunch xsens_mti_driver xsens_mti_node.launch 此时可以接收到雷达和IMU的topic了，使用rosbag record ***.bag &lt;topic名字&gt;"},{"title":"vim使用总结","date":"2021-02-04T01:43:39.000Z","path":"2021/02/04/vim使用总结/","text":"在vim中多开窗口并切换 (1) use :sp or :vs to split the windows(2) press the ctrl+w firstly, then press h to switch to the left window (in the same way, ctrl+w j to the down window, ctrl+w k to the up window, ctrl+w l to the right window) markdownif I want to view the markdown file (1) install the &quot;markdown-preview.vim&quot; plugin (2) use :MarkdownPreview to view it in the browser move the cursor(1) move the cursor to the first line of a file :0 or :1 gg (2) move the cursor to the last line of a file : G shift+g (3) move the cursor to the head of a line Home ^ also shift+6 0 (4) move the cursor to the tail of a line End shift+4 to move the cursor to the tail of the current line (5) shortcuts: search:/&lt;word you search&gt; (6) open fileif vim is not launched: vim &lt;file1_name&gt; &lt;file2_name&gt; &lt;file3_name&gt; ...if vim is launched: :open &lt;file_name&gt; (7) switch files in the opened filesswitch files in the last two opened files: ctrl + 6next file :bnlast file :bp (8) revokerevoke the last operation :n n u recover the revoke operation `ctrl + r`"},{"title":"SVD奇异值分解","date":"2021-02-02T12:14:55.000Z","path":"2021/02/02/SVD奇异值分解/","text":"奇异值分解是线性代数中一种重要的矩阵分解方法，本文通过一个具体的例子来说明如何对一个矩阵A进行奇异值分解。 首先，对于一个m*n的矩阵，如果存在正交矩阵U(m*m阶)和V(n*n阶)，使得(1)式成立： \\begin{equation} \\begin{split} A &= UΣV^T \\end{split} \\end{equation}\\tag{1}则将式(1)的过程称为奇异值分解，其中$Σ_{mn}=\\begin{bmatrix}a &amp; b\\\\c &amp; d\\end{bmatrix}$，且$Σ_1=diag(σ_1,σ_2,…,σ_r)$，U和V分别称为A的左奇异向量矩阵和右奇异向量矩阵。下面用一个具体的例子来说明如何得到上述的分解。 对于矩阵$A=\\begin{bmatrix}1&amp;1\\\\1&amp;1\\\\0&amp;0\\end{bmatrix}$, 第一步计算U，计算矩阵$AA^T=\\begin{bmatrix}2&amp;2&amp;0\\\\2&amp;2&amp;0\\\\0&amp;0&amp;0\\end{bmatrix}$，对其进行特征分解，分别得到特征值4，0，0和对应的特征向量$[\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0]^T$,$[-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0]^T$,$[0,0,1]^T$，可以得到$U=\\begin{bmatrix}\\frac{1}{\\sqrt{2}}&amp;-\\frac{1}{\\sqrt{2}}&amp;0\\\\\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{\\sqrt{2}}&amp;0\\\\0&amp;0&amp;1\\end{bmatrix}$ 第二步计算V，$A^TA=\\begin{bmatrix}2&amp;2\\\\2&amp;2\\end{bmatrix}$，对其进行特征分解，分别得到特征值4，0和对应的特征向量$[\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}]^T$，$[-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}]^T$，可以得到$V=\\begin{bmatrix}\\frac{1}{\\sqrt{2}}&amp;-\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{\\sqrt{2}}\\end{bmatrix}$ 第三部计算$Σ_{mn}=\\begin{bmatrix}Σ_1&amp;0\\\\0&amp;0\\end{bmatrix}$，其中$Σ_1=diag(σ_1,σ_2,…,σ_r)$是将第一或第二步求出的非零特征值从大到小排列后开根号的值，这里$Σ=\\begin{bmatrix}2&amp;0\\\\0&amp;0\\\\0&amp;0\\end{bmatrix}$ 最终，我们可以得到A的奇异值分解 \\begin{equation} \\begin{split} A &= UΣV^T= \\begin{bmatrix}\\frac{1}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}&0\\\\\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0\\\\0&0&1\\end{bmatrix} \\begin{bmatrix}2&0\\\\0&0\\\\0&0\\end{bmatrix} \\begin{bmatrix}\\frac{1}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\\end{bmatrix} ^T=\\begin{bmatrix}1&1\\\\1&1\\\\0&0\\end{bmatrix} \\end{split} \\end{equation} 矩阵的特征值分解和奇异值分解有什么区别？ 首先，特征值只能作用在一个m*m的正方矩阵上，而奇异值分解则可以作用在一个m*n的长方矩阵上。其次，奇异值分解同时包含了旋转、缩放和投影三种作用，(1)式中，U和V都起到了对A旋转的作用，而Σ起到了对A缩放的作用。特征值分解只有缩放的效果。"},{"title":"k-core算法","date":"2021-01-28T09:09:15.000Z","path":"2021/01/28/k-core算法/","text":"k-core算法在图论中，k-degenerate（k简并）图是一种每个子图都有一个度不超过k的顶点的无向图，即子图中的某个顶点接触子图的k条或更少的边。图的degeneracy（简并度）是k简并的k的最小值。图的简并度是对它的稀疏程度的度量，并且是在其他稀疏度量的常数因子内，例如图的树状度。 简并度也被称为k-core数（num）、宽度（width）、和连锁（linkage），本质上与着色数（coloring number）或Szekeres-Wilf number（以Szekeres和Wilf(1968)命名）相同。k简并图也被称为k-inductive（k归纳）图。图的简并度可以通过反复删除最小度顶点的算法在线性时间内计算。去掉所有小于k度的顶点后剩下的连通分量称为图的k-cores（k核），图的简并度为具有k核的k的最大值。 如图所示是一张2-core图，每个顶点在其左边最多有两个邻居，因此任何子图的最右顶点的度最多为2。它的2核，即反复删除度数小于2的顶点后剩下的子图，是阴影的。 正如Matula &amp; Beck（1983）所描述的，在线性时间内，通过使用一个桶队列反复寻找并移除最小度的顶点，可以找到优化排序着色数的有限图G的顶点排序。那么简并度就是任何顶点在被移除时的最高度。设图中的节点数为n。更详细地说，该算法是这样进行的： 初始化一个输出列表L。 对G中的每个顶点v计算一个数字dv，即v不存在于l中的邻居数。最初，这些数字只是顶点的度数。 初始化一个数组D，使D[i]包含一个不在L中dv = i的顶点v的列表。 将k初始化为0。 重复n次: 扫描阵列单元D[0]， D[1]，…直到找到一个D[i]非空的i为止。 从D[i]中选择一个顶点v。在L的开头加上v，然后从D[i]中删除v。 对于v的每一个未在L中的相邻w，从dw中减去1，然后将w移到对应dw新值的D单元中。 在算法的最后，k包含G的简并性，L包含一个顶点列表，该顶点以最优的着色数排序。G的i核是L的前缀，由k先取大于等于i的值后添加到L的顶点组成。初始化变量L, dv, D和k可以很容易地在线性时间内完成。查找每个连续移除的顶点v，并调整包含v的邻居的D的单元，所花费的时间与该步骤dv的值成正比；但是这些值的总和就是图上的边的数量(每条边对后面的顶点的总和都有贡献)，所以总时间是线性的。"},{"title":"Boss直聘折腾","date":"2021-01-28T08:59:31.000Z","path":"2021/01/28/Boss直聘折腾/","text":"Boss直聘折腾记录非出于本心，需要获得用户的简历（内心一百万个不愿意）。。。本人绝不使用这些数据用于任何研究，手中也绝不留存任何信息，仅当技术学习，如有侵权，务必直接联系本人。 第一步登录boss直聘企业账号，发布职位，即可获取推荐牛人。 首先直接考虑访问url，不过boss直聘的反扒策略肯定不允许如此简单的方法，页面内容都是由Cookie生成的，所以此方法行不通。 其次考虑selenium控制浏览器，不过这种方法每次都需要打开新页面，就得重复扫码登录，无法完成自动化运行，舍弃该方法。 最后经过M老师指点，采用TamperMonkey的方法，在其中编写JavaScript脚本，截取相关代码，并模拟点击行为，最后在chrome的console中成功获取所需内容。为了将console中的内容保存到本地，采用了命令行google-chrome --enable-logging --v=1保存chrome console log文件，这样就可以下一步的数据提取了，使用Xml、BeautifulSoup等方法。为了保护相关人员权益，避免风险，不公开JavaScript脚本，恳请见谅。 在整个过程中，还是学习到JavaScript、TamperMonkey的知识，感谢M老师和震哥的指点。"},{"title":"github用户分析","date":"2021-01-25T08:22:50.000Z","path":"2021/01/25/github用户分析/","text":"项目介绍Github作为全球最大的开源代码托管平台，存在着非常多的技术人员和宝贵的项目，这些都是非常难得的分析资源，对它们进行分析具有非常高的价值。 用户分析鉴于Github用户过多，进行全量的分析是不现实的。由于最近团队在洞察Rust社区，选择对其中的贡献者进行分析。首先需要获得所有的Rust贡献者，这里有两种方法获得。 （1）爬取Crates.io（Rust社区所有crates的介绍网站）上的所有贡献者信息，可以有API来获取。不过这种方法需要进行长时间的爬取，比较繁琐。 （2）Rust官方有一个thanks仓库，里面可以获得所有的贡献者名称。可以按照该仓库的使用介绍来获得ALL-TIME的贡献者名称。 得到具体的用户login后，可以调用Github API来获得贡献者的基本信息和相关关系。其中用户间的的关系我首先使用了following关系（后续可以添加follows、sponsors、sponsoring关系等），使用https://api.github.com/users/&lt;user-login&gt;/following来下载关系信息，使用https://api.github.com/users/&lt;user-login&gt;来下载基本信息。 得到用户的基本信息和相互的关系信息后，以人为节点建立知识图谱。个人偏好的工具有Networkx和Neo4j，都是非常好用的软件，其中Networkx可以使用python调用，Neo4j具有一套自己的图数据库系统。简单进行分析的话我选择Networkx。 从基本信息文件中抽取login作为节点名称，id、node_id、name、compony、blog、location、email、twitter_username、public_repos、public_gists、followers、following、bio、created_at、updated_at、avatar_url、html_url作为节点属性，建立单个节点。 从关系信息文件中抽取following关系作为两个节点之间的联系。"},{"title":"在linux终端优雅地打印JSON文件","date":"2021-01-25T07:30:53.000Z","path":"2021/01/25/在linux终端优雅地打印JSON文件/","text":"JSON 文件非常棒，因为它们以人类可读的格式存储数据集合。然而，如果 JSON 文件被最小化过，那么阅读 JSON 文件可能会很痛苦。以下面的JSON文件为例。 &#123;&quot;login&quot;: &quot;gperinazzo&quot;, &quot;id&quot;: 9922687, &quot;node_id&quot;: &quot;MDQ6VXNlcjk5MjI2ODc=&quot;, &quot;avatar_url&quot;: &quot;https://avatars0.githubusercontent.com/u/9922687?v=4&quot;, &quot;gravatar_id&quot;: &quot;&quot;, &quot;url&quot;: &quot;https://api.github.com/users/gperinazzo&quot;, &quot;html_url&quot;: &quot;https://github.com/gperinazzo&quot;, &quot;followers_url&quot;: &quot;https://api.github.com/users/gperinazzo/followers&quot;, &quot;following_url&quot;: &quot;https://api.github.com/users/gperinazzo/following&#123;/other_user&#125;&quot;, &quot;gists_url&quot;: &quot;https://api.github.com/users/gperinazzo/gists&#123;/gist_id&#125;&quot;, &quot;starred_url&quot;: &quot;https://api.github.com/users/gperinazzo/starred&#123;/owner&#125;&#123;/repo&#125;&quot;, &quot;subscriptions_url&quot;: &quot;https://api.github.com/users/gperinazzo/subscriptions&quot;, &quot;organizations_url&quot;: &quot;https://api.github.com/users/gperinazzo/orgs&quot;, &quot;repos_url&quot;: &quot;https://api.github.com/users/gperinazzo/repos&quot;, &quot;events_url&quot;: &quot;https://api.github.com/users/gperinazzo/events&#123;/privacy&#125;&quot;, &quot;received_events_url&quot;: &quot;https://api.github.com/users/gperinazzo/received_events&quot;, &quot;type&quot;: &quot;User&quot;, &quot;site_admin&quot;: false, &quot;name&quot;: null, &quot;company&quot;: null, &quot;blog&quot;: &quot;&quot;, &quot;location&quot;: null, &quot;email&quot;: &quot;gperinazzo@gmail.com&quot;, &quot;hireable&quot;: null, &quot;bio&quot;: null, &quot;twitter_username&quot;: null, &quot;public_repos&quot;: 21, &quot;public_gists&quot;: 0, &quot;followers&quot;: 4, &quot;following&quot;: 0, &quot;created_at&quot;: &quot;2014-11-24T01:48:29Z&quot;, &quot;updated_at&quot;: &quot;2020-11-08T00:34:34Z&quot;&#125; 计算机可以很容易地读取它。即使是人也能读，但如果 JSON 文件以合适的格式显示，那么阅读就会简单很多。理想中的 JSON 文件应该是这样读的： &#123; &quot;login&quot;: &quot;gperinazzo&quot;, &quot;id&quot;: 9922687, &quot;node_id&quot;: &quot;MDQ6VXNlcjk5MjI2ODc=&quot;, &quot;avatar_url&quot;: &quot;https://avatars0.githubusercontent.com/u/9922687?v=4&quot;, &quot;gravatar_id&quot;: &quot;&quot;, &quot;url&quot;: &quot;https://api.github.com/users/gperinazzo&quot;, &quot;html_url&quot;: &quot;https://github.com/gperinazzo&quot;, &quot;followers_url&quot;: &quot;https://api.github.com/users/gperinazzo/followers&quot;, &quot;following_url&quot;: &quot;https://api.github.com/users/gperinazzo/following&#123;/other_user&#125;&quot;, &quot;gists_url&quot;: &quot;https://api.github.com/users/gperinazzo/gists&#123;/gist_id&#125;&quot;, &quot;starred_url&quot;: &quot;https://api.github.com/users/gperinazzo/starred&#123;/owner&#125;&#123;/repo&#125;&quot;, &quot;subscriptions_url&quot;: &quot;https://api.github.com/users/gperinazzo/subscriptions&quot;, &quot;organizations_url&quot;: &quot;https://api.github.com/users/gperinazzo/orgs&quot;, &quot;repos_url&quot;: &quot;https://api.github.com/users/gperinazzo/repos&quot;, &quot;events_url&quot;: &quot;https://api.github.com/users/gperinazzo/events&#123;/privacy&#125;&quot;, &quot;received_events_url&quot;: &quot;https://api.github.com/users/gperinazzo/received_events&quot;, &quot;type&quot;: &quot;User&quot;, &quot;site_admin&quot;: false, &quot;name&quot;: null, &quot;company&quot;: null, &quot;blog&quot;: &quot;&quot;, &quot;location&quot;: null, &quot;email&quot;: &quot;gperinazzo@gmail.com&quot;, &quot;hireable&quot;: null, &quot;bio&quot;: null, &quot;twitter_username&quot;: null, &quot;public_repos&quot;: 21, &quot;public_gists&quot;: 0, &quot;followers&quot;: 4, &quot;following&quot;: 0, &quot;created_at&quot;: &quot;2014-11-24T01:48:29Z&quot;, &quot;updated_at&quot;: &quot;2020-11-08T00:34:34Z&quot; &#125; 可以使用大多数的文本编辑器和一些插件以合适的格式显示它。然而，如果在终端中，或者想在 shell 脚本中这么做，事情会有所不同。对于一个已最小化过的 JSON 文件，有同样的办法在 Linux 终端中漂亮地输出它。 jq 是一个命令行 JSON 处理器。可以用它来切分、过滤、映射和转换结构化数据。可以使用 apt 命令在 Ubuntu 上安装它： sudo apt install jq jq . ***.json 或者 cat ***.json | jq 如果想用漂亮的格式修改原来的 JSON 文件，可以把解析后的输出结果用管道传送到一个新的文件中，然后覆盖原来的 JSON 文件。 jq . ***.json &gt; ###.json 对一个格式良好的 JSON 文件进行最小化，可以使用选项 -c。 jq -c &lt; ###.json 如果系统中安装有python3，可以再终端中使用以下命令： python3 -m json.tool ***.json"},{"title":"keyword_extraction","date":"2021-01-12T06:14:54.000Z","path":"2021/01/12/keyword-extraction/","text":"关键词提取简介关键字提取(又称关键字检测或关键字分析)是一种文本分析技术，可以自动从文本中提取最常用、最重要的单词和表达式，它有助于总结文本内容和识别讨论的主要话题。 文本分析使用带有自然语言处理(NLP)的机器学习人工智能(AI)来分解人类语言，使其能够被机器理解和分析。关键字分析可以从各种文本中找到关键字：常规文件和商业报告，社交媒体评论，在线论坛和评论，新闻报道，等等。假设您想分析关于您的产品的数千条在线评论。关键字提取可以帮助您筛选整个数据集，并在几秒钟内获得最适合描述每个评论的词。通过这种方式，您可以很容易地自动地看到您的客户最经常提到的内容，从而为您的团队节省数小时的手工处理时间。 有一些免费的词云生成器可以比较直观地体现关键词，例如Word clouds。 方法 Simple Statistical Approaches 主要包括词频、词搭配和共现、TF-IDF(术语频率反向文档频率的缩写)和RAKE(快速自动关键字提取)，基于统计的方法只依赖于统计数据，可能会忽略那些只被提到一次但仍然应该被认为是相关的相关单词或短语。 1、词频 词频包括列出在一篇文章中重复次数最多的单词和短语。这对于很多目的都很有用，从在一组产品评审中识别反复出现的术语，到发现客户支持交互中最常见的问题。然而，词频方法认为文档只是一个“单词袋”，而不考虑与含义、结构、语法和单词序列相关的关键方面。例如，这种关键字提取方法无法检测到同义词，从而忽略了非常有价值的信息。 2、单词搭配和共现 也被称为N-gram统计，单词搭配和共现有助于理解文本的语义结构，并将多个单词算作一个单词。搭配是指经常放在一起的词。最常见的搭配类型是bi-grams(两个相邻的词，如“客户服务”、“视频电话”或“电子邮件通知”)和trig(三个词组成的组，如“易于使用”或“社交媒体渠道”)。另一方面，共现现象是指在同一语料库中倾向于共现的单词。它们不一定是相邻的，但它们在语义上确实具有相似性。 3、TF-IDF TF-IDF表示词频-逆文档频率，用来衡量文档集合中一个单词对文档的重要性。该度量计算单词在文本中出现的次数(术语频率)，并将其与逆向文档频率(该单词在整个数据集中的罕见或常见程度)进行比较，将这两个量相乘就提供了文档中单词的TF-IDF分数。分数越高，这个词就越与文档相关。当涉及到关键字提取时，这个指标可以帮助您识别文档中最相关的词(得分较高的词)，并将它们视为关键词。 4、RAKE 快速自动关键字提取(RAKE)是一种著名的关键字提取方法，它使用stopwords和短语分隔符列表来检测一段文本中最相关的单词或短语，以以下文本为例:Keyword extraction is not that difficult after all. There are many libraries that can help you with keyword extraction. Rapid automatic keyword extraction is one of those.该方法所做的第一件事是将文本拆分为一个单词列表，并从该列表中删除stopwords。这将返回一个所谓内容词的列表。假设我们的stopwords和短语分隔符列表如下:stopwords = [is, not, that, there, are, can, you, with, of, those, after, all, one] 分隔符=[., ,]然后，我们的8个内容词的列表将像这样:content_words =[keyword, extraction, difficult, many, libraries, help, rapid, automatic]然后，该算法以短语分隔符和停止词分隔文本，以创建候选表达式。所以，候选人的关键短语如下:Keyword extraction is not that difficult after all. There are many libraries that can help you with keyword extraction. Rapid automatic keyword extraction is one of those.一旦文本被分割，该算法就会创建一个单词共现矩阵。每一行显示一个给定的内容词与候选短语中每个其他内容词共现的次数。对于上面的例子，矩阵看起来像这样: 在这个矩阵建立之后，单词会得到一个分数。这一点可以计算为一个词的程度在矩阵(即共生的数量的总和词与其他词在文本内容),随着词频的次数(即这个词出现在文本),或作为其频率除以这个词的程度。如果我们计算每个单词的学位分数除以频率分数，它们看起来会像这样: 这些表达式也会给出一个分数，这个分数是每个单词的分数之和。如果我们要计算上面粗体部分的得分，它们会是这样的: 如果两个关键字或关键字短语以相同的顺序同时出现两次以上，不管关键字短语在原始文本中包含多少stopwords，都会创建一个新的关键字短语。该关键字的得分计算方式与单个关键字的得分计算方式相同。如果一个关键字或关键字短语的分数属于top T分数，其中T是您想要提取的关键字的数目，则选择该关键字或关键字短语。根据原始论文，T默认了文件中三分之一的内容词。对于上面的示例，该方法将返回前3个关键字，根据我们定义的分数，这将是rapid automatic keyword extraction(13.33)、keyword extraction(5.33)和many libraries(4.0)。 语言学方法 关键词提取方法通常利用关于文本及其包含的单词的语言信息。有时，使用词法或句法信息(例如单词的词性或句子的依赖语法表示中单词之间的关系)来确定应该提取哪些关键字。在某些情况下，某些PoS会得到更高的分数(例如名词和名词短语)，因为它们通常比其他类别包含更多关于文本的信息。 图论方法 基于图的关键字提取的基本思想总是相同的:根据从图的结构中获取的一些信息来度量顶点的重要性，从而提取最重要的顶点。一旦建立了一个图，就该确定如何度量顶点的重要性了。有许多不同的选择，其中大部分在本文中处理。有些方法选择测量顶点的度数。顶点的度等于边的数量或连接顶点的土地(也称为度)+边的数量开始的顶点(也称为度)除以最大度(=图的顶点数- 1)。这个公式来计算一个顶点的度: Dv = (Dvin + Dvout) / (N - 1) 其他一些方法测量到一个给定顶点的直接顶点的数量(称为邻域大小)。 无论选择的度量是什么，每个顶点都会有一个分数，它将决定是否将其作为关键字提取。以以下文本为例:Automatic1 graph-based2 keyword3 extraction4 is pretty5 straightforward6. A document7 is represented8 as a graph9 and a score10 is given11 to each of the vertices12 in the graph13. Depending14 on the score15 of a vertex16, it might be chosen17 as a keyword18.如果我们衡量社区大小上面的示例图的依赖性只包含有实词(编号1 - 18在文本),提取的关键词会被自动等基于关键字提取自附近的大小头名词提取(等于3/17)是最高的。 机器学习方法 1、条件随机域条件随机场(CRF)是一种统计方法，通过对文本中出现的单词序列的不同特征进行加权来学习模式。这种方法考虑上下文和不同变量之间的关系，以便作出预测。使用条件随机场允许您创建复杂和丰富的模式。这种方法的另一个优点是它的泛化能力:一旦模型用某个领域的例子进行了训练，它就可以很容易地将它所学到的知识应用到其他领域。缺点是，为了使用条件随机场，您需要有很强的计算能力来计算所有单词序列的所有特征的权重。 评估关键字提取器的性能在评估关键字提取器的性能时，您可以使用机器学习中的一些标准指标:准确性、精度、召回率和F1分数。然而，这些指标并不能反映部分匹配;他们只考虑所提取的片段与该标记的正确预测之间的完美匹配。幸运的是，还有一些其他指标能够捕获部分匹配。一个例子就是ROUGE。ROUGE(面向回忆的基sting评价替补研究)是一组比较源文本和提取词之间不同参数(如重叠词的数量)的指标。参数包括序列的长度和数目，可以手动定义。 混合方法 混合之前的方法。"},{"title":"nlp工具总结","date":"2021-01-12T02:44:43.000Z","path":"2021/01/12/nlp工具总结/","text":"NLP工具总结 MonkeyLearn相关解决方案"},{"title":"mysql使用总结","date":"2021-01-12T02:29:10.000Z","path":"2021/01/12/mysql使用总结/","text":"MySQL使用总结"},{"title":"rust使用总结","date":"2021-01-11T15:03:15.000Z","path":"2021/01/11/rust使用总结/","text":"rust使用总结 Rust Report Card - 为你的 Rust 代码生成质量报告 简介： 官网https://rust-reportcard.xuri.me，Rust Report Card 服务，通过近 500 个代码检测项帮助分析你的 Rust 代码中的潜在质量问题，还可获得 badge 徽章。 该服务基于 rust-clippy 中 定义的 lint 规则对 Rust 进行静态代码分析扫描，在 Rust 编译器检查的基础之上帮你更进一步，编写出更好的代码，检查内容涵盖可优化性能写法、可简化代码写法、代码惯用风格更符合最佳实践、未使用变量和 cargo 声明文件检查等。 Rust Report Card 提供了增量代码扫描、代码质量评级、徽章生成、协议检查和内部开源 Rust 代码质量评分排行等功能。 代码质量评级与得分规则： 3+ 评级 得分 Badge A+ &gt; 90 A &gt; 80 B &gt; 70 C &gt; 60 D &gt; 50 E &gt; 40 F &lt;= 40"},{"title":"typescript使用总结","date":"2021-01-11T13:46:51.000Z","path":"2021/01/11/typescript使用总结/","text":"Type Script使用总结 安装typescript sudo apt install npm sudo npm install -g typescript 通过tsc命令确认安装完成 tsc -v 运行ts文件 tsc ***.ts // 得到***.js文件 node ***.js // 使用node命令执行js文件 运行ts文件出现以下错误 Cannot find module &#39;fs&#39; 执行以下命令： sudo npm install @types/node"},{"title":"python使用总结","date":"2021-01-09T14:20:08.000Z","path":"2021/01/09/python使用总结/","text":"python使用总结 python numpy real函数 用法： numpy.real(val) 返回复杂参数的实部。 | 参数： | val**： ： array_like输入数组。 || ———— | —————————————————————————————— || 返回值： | out**： ： ndarray或标量复杂参数的实际组成部分。如果val是实数，则将val的类型用于输出。如果val具有复杂元素，则返回的类型为float。 | 例子： &gt;&gt;&gt; a = np.array([1+2j, 3+4j, 5+6j]) &gt;&gt;&gt; a.real array([1., 3., 5.]) &gt;&gt;&gt; a.real = 9 &gt;&gt;&gt; a array([9.+2.j, 9.+4.j, 9.+6.j]) &gt;&gt;&gt; a.real = np.array([9, 8, 7]) &gt;&gt;&gt; a array([9.+2.j, 8.+4.j, 7.+6.j]) &gt;&gt;&gt; np.real(1 + 1j) 1.0 源码： numpy.real的API实现见：[源代码] python执行ubuntu terminal命令 import os os.system(&#39;&lt;ubuntu 命令&gt;&#39;) 在执行ubuntu terminal命令时，如果需要切换路径： os.chdir(&#39;&lt;路径名称&gt;&#39;)"},{"title":"ubuntu使用总结","date":"2021-01-09T08:21:12.000Z","path":"2021/01/09/ubuntu使用总结/","text":"ubuntu 使用总结 ubuntu查看环境变量有三个命令 1、env env命令是environment的缩zhi写，用dao于列出所zhuan有的环境变量 2、export 单独使用export命令也可以像env列出所有的环境变量，不过export命令还有其他额外的功能 3、echo $PATH echo $PATH用于列出变量PATH的值，里面包含了已添加的目录 ubuntu修改环境变量： 1、使用set命令 set查看所有本地定义的环境变量，unset可以删除指定的环境变量。 2、使用export命令 export &lt;变量名称&gt;=$&lt;变量名称&gt;:&lt;变量值&gt; //等号两边一定不能有空格 3、修改.bashrc文件 vi ~/.bashrc //编辑.bashrc文件 //在最后一行添上: export &lt;变量名称&gt;=$&lt;变量名称&gt;:&lt;变量值&gt; //等号两边没空格 //保存退出 source ~/.bashrc 注意，常用的环境变量： PATH 决定了shell将到哪些目录中寻找命令或程序 HOME 当前用户主目录 HISTSIZE 历史记录数 LOGNAME 当前用户的登录名 HOSTNAME 指主机的名称 SHELL 当前用户Shell类型 LANGUGE 语言相关的环境变量，多语言可以修改此环境变量 MAIL 当前用户的邮件存放目录 PS1 基本提示符，对于root用户是#，对于普通用户是$ ubuntu中&gt;和&gt;&gt;和&lt;的区别 1、输入输出的区bai别： &gt;&gt;和&gt;都属于输出重定向，&lt;属于输入重定向。 2、文件内容的区别： &gt;会覆盖目标的原有内容。当文件存在时会先删除原文件，再重新创建文件，然后把内容写入该文件；否则直接创建文件。 &gt;&gt;会在目标原有内容后追加内容。当文件存在时直接在文件末尾进行内容追加，不会删除原文件；否则直接创建文件。 date命令用法：date [选项]… [+格式] 或：date [-u|—utc|—universal] [MMDDhhmm[[CC]YY][.ss]]以给定的格式显示当前时间，或是设置系统日期。 -d，—date=字符串 显示指定字符串所描述的时间，而非当前时间 -f，—file=日期文件 类似—date，从日期文件中按行读入时间描述 -r,，—reference=文件 显示文件指定文件的最后修改时间 -R，—rfc-2822 以RFC 2822格式输出日期和时间 例如：2006年8月7日，星期一 12:34:56 -0600 --rfc-3339=TIMESPEC 以RFC 3339 格式输出日期和时间。 TIMESPEC=&#39;date&#39;，&#39;seconds&#39;，或 &#39;ns&#39; 表示日期和时间的显示精度。 日期和时间单元由单个的空格分开： 2006-08-07 12:34:56-06:00 -s，—set=字符串 设置指定字符串来分开时间 -u，—utc, —universal 输出或者设置协调的通用时间 --help 显示此帮助信息并退出 --version 显示版本信息并退出 给定的格式FORMAT 控制着输出，解释序列如下： %% 一个文字的 % %a 当前locale 的星期名缩写(例如： 日，代表星期日) %A 当前locale 的星期名全称 (如：星期日) %b 当前locale 的月名缩写 (如：一，代表一月) %B 当前locale 的月名全称 (如：一月) %c 当前locale 的日期和时间 (如：2005年3月3日 星期四 23:05:25) %C 世纪；比如 %Y，通常为省略当前年份的后两位数字(例如：20) %d 按月计的日期(例如：01) %D 按月计的日期；等于%m/%d/%y %e 按月计的日期，添加空格，等于%_d %F 完整日期格式，等价于 %Y-%m-%d %g ISO-8601 格式年份的最后两位 (参见%G) %G ISO-8601 格式年份 (参见%V)，一般只和 %V 结合使用 %h 等于%b %H 小时(00-23) %I 小时(00-12) %c 按年计的日期(001-366) %k 时(0-23) %l 时(1-12) %m 月份(01-12) %M 分(00-59) %n 换行 %N 纳秒(000000000-999999999) %p 当前locale 下的”上午”或者”下午”，未知时输出为空 %P 与%p 类似，但是输出小写字母 %r 当前locale 下的 12 小时时钟时间 (如：11:11:04 下午) %R 24 小时时间的时和分，等价于 %H:%M %s 自UTC 时间 1970-01-01 00:00:00 以来所经过的秒数 %S 秒(00-60) %t 输出制表符 Tab %T 时间，等于%H:%M:%S %u 星期，1 代表星期一 %U 一年中的第几周，以周日为每星期第一天(00-53) %V ISO-8601 格式规范下的一年中第几周，以周一为每星期第一天(01-53) %w 一星期中的第几日(0-6)，0 代表周一 %W 一年中的第几周，以周一为每星期第一天(00-53) %x 当前locale 下的日期描述 (如：12/31/99) %X 当前locale 下的时间描述 (如：23:13:48) %y 年份最后两位数位 (00-99) %Y 年份 %z +hhmm 数字时区(例如，-0400) %:z +hh:mm 数字时区(例如，-04:00) %::z +hh:mm:ss 数字时区(例如，-04:00:00) %:::z 数字时区带有必要的精度 (例如，-04，+05:30) %Z 按字母表排序的时区缩写 (例如，EDT) 默认情况下，日期的数字区域以0 填充。以下可选标记可以跟在”%”后: - (连字符)不填充该域 _ (下划线)以空格填充 0 (数字0)以0 填充 ^ 如果可能，使用大写字母 # 如果可能，使用相反的大小写 在任何标记之后还允许一个可选的域宽度指定，它是一个十进制数字。作为一个可选的修饰声明，它可以是E，在可能的情况下使用本地环境关联的表示方式；或者是O，在可能的情况下使用本地环境关联的数字符号。"},{"title":"git使用总结","date":"2021-01-09T08:18:40.000Z","path":"2021/01/09/git使用总结/","text":"git 使用总结 git log --stat的使用 1、显示最近一次被修改文件的修改：统计信息，添加或删除了多少行。 git log -1 --stat 2、显示最近两条的修改 git log --stat -2 3、显示具体的修改 git log -p -2 4、显示用户为czd的修改 git log --stat --author=czd 5、查看单个文件sensor-dev.c最近三次修改的记录 git log --stat -3 drivers/input/sensors/sensor-dev.c 6、如果想不分页输出全部log日志 git --no-pager log 当使用git --no-pager log --stat &gt;&gt; temp.txt出现以下报错信息时： warning: inexact rename detection was skipped due to too many files. warning: you may want to set your diff.renameLimit variable to at least 6567 and retry the command. 修改git的diff.renameLimit变量值 git config diff.renameLimit 2000000 github api使用总结 1、得到某个仓库的贡献者 api.github.com/repos/&lt;项目所属组织或个人&gt;/&lt;项目名称&gt;/contributors 返回的json中contributions即为commit次数。 2、得到某一用户在github上面贡献了多少库 benjycui编写了一个实现该功能的工具。 修改git commit的默认编辑器 进入项目的根目录下，使用vim ./.git/config and add the &quot;editor = vim&quot;或git config --global core.editor vim将默认编辑器从GNA nano修改为vim。 新建一个仓库并和远程仓库连接 1、mkdir new_repo 2、cd new_repo 3、git init 4、在github或gitee上新建一个同名仓库 5、git remote add &lt;remote_repo_name&gt; &lt;remote_repo_address&gt; 分支 1、查看所有分支 git branch 2、选择某个特定的分支 git checkout &lt;branch name&gt; 3、新建一个分支并选定 git checkout -b &lt;branch name&gt;"},{"title":"ubuntu服务器-Nginx初步配置及站点部署","date":"2021-01-08T01:51:13.000Z","path":"2021/01/08/ubuntu服务器-Nginx初步配置及站点部署/","text":"ubuntu服务器-Nginx初步配置及站点部署 使用SSH远程登录服务器 ssh root@***.***.***.*** 安装Nginx并启动服务器 apt update apt install nginx systemctl start nginx 将项目上传到服务器 可以选择本地上传或者 github clone。 编辑默认的default root /var默认路径，并将代码移动至路径下。 cd /etc/nginx/sites-available vim default (rewrite) root /var/www/sites cd /var/www mv &lt;code path&gt; sites 检测nginx配置文件，重启nginx服务器 nginx -t service nginx restart"},{"title":"基于Github和Hexo搭建个人网站","date":"2021-01-06T12:13:18.000Z","path":"2021/01/06/website-based-github-hexo/","text":"基于Github和Hexo搭建个人网站 安装并配置 Git Git 是目前世界上最先进的分布式版本控制系统（没有之一）,使用 Git 的目的是为了将网站从本地提交上服务器（GitHub）上面去。具体的 Git 操作细节可以查看廖雪峰的教程，讲的十分详细。 在终端进行操作，设置 user.name 和 user.email 配置信息，为了方便推荐设置为全局。 git config --global user.name &quot;GitHub账号名称&quot; git config --global user.email &quot;GitHub注册邮箱&quot; ** 通过注册的邮箱生成 ssh 密钥文件： ssh-keygen -t rsa -C &quot;GitHub注册邮箱&quot; 直接三个回车，默认不需要设置密码。最后得到了两个文件： id_rsa 和 id_rsa.pub。打开 id_rsa.pub 文件，将里面的内容全部复制到 Github 上。添加 SSH key 后，进行测试。 ssh -T git@github.com 你将会看到： The authenticity of host &#39;github.com (207.97.227.239)&#39; can&#39;t be established. RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. Are you sure you want to continue connecting (yes/no)? 选择yes后 Hi ***! You&#39;ve successfully authenticated, but GitHub does not provide shell access. 新建一个 Github 仓库，仓库名为：&lt;github账号名称&gt;.github.io 安装Node.js可以根据自己的系统自行搜索安装方法，ubuntu 上的安装方法为：sudo apt-get install npm sudo apt-get install nodejs 安装并配置 Hexo Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 sudo npm install -g hexo-cli 初始化博客 hexo init myBlog 进入myBlog文件夹中，输入 hexo server 或者hexo s 打开浏览器输入地址：localhost:4000 这样就可以看见博客了。 添加文章(1) 可以将你平时写的文章直接导入到 _posts 文件夹里，注意文章类型得是 md 格式。(2) 可以写新文章，执行以下命令 hexo new [layout] &lt;title&gt; 可以在命令中指定文章的布局（layout），默认为 post，可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局。 生成静态网页 hexo generate 或者hexo g 将网页部署到 Github 上Hexo 提供了快速方便的一键部署功能，只需一条命令就能将网站部署到服务器上。在开始之前，必须先在 _config.yml 中修改参数，一个正确的部署配置中至少要有 type 参数，例如：deploy: type: git repo: https://github.com/***/***.github.io branch: master 之后输入以下命令：hexo deploy 或者hexo d 稍等片刻，网站就已经部署好了，可以在浏览器输入 ***.github.io，这样一个免费的博客就已经搭建好了。 选择主题Hexo 默认的主题比较一般，可以在Hexo的主题网站进行主题的挑选更换。将下好的主题安放在themes文件夹内，同时在_config.yml中进行主题修改就好了。一般每个主题都会有比较详细的说明文档，按照文档进行修改即可。 Hexo渲染LaTeX公式Hexo渲染主题的两个重要因素：mathjax和kramed，前者是数学公式渲染引擎，后者是Hexo的markdown渲染引擎，hexo默认渲染引擎是marked，但是它不支持mathjax，因此需要替换引擎。 一、Hexo添加mathjax 如果hexo安装有hexo-math,需要先卸载它。卸载命令： npm uninstall hexo-math --save 安装mathjax，安装命令： npm install hexo-renderer-mathjax --save hexo主题开启mathjax： 进入主题目录，编辑_config.yml，开启mathjax： # MathJax Support mathjax: enable: true per_page: true 二、hexo切换kramed引擎 卸载marked引擎 npm uninstall hexo-renderer-marked --save 安装kramed引擎 npm install hexo-renderer-kramed --save 修改引擎bug 修改文件/node_modules\\kramed\\lib\\rules\\inline.js中escape和em两行，具体修改如下： // escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()#$+\\-.!_&gt;])/, escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/, 这一步是在原基础上取消了对,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 进入主题目录，编辑_config.yml，开启mathjax： // em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 重新启动hexo: hexo clean &amp;&amp; hexo g -d"}]